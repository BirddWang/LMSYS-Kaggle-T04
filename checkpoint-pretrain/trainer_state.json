{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 5239,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.001908761213972132,
      "grad_norm": 161.34584045410156,
      "learning_rate": 4e-05,
      "loss": 3.624,
      "step": 10
    },
    {
      "epoch": 0.003817522427944264,
      "grad_norm": 46.42463684082031,
      "learning_rate": 0.00014,
      "loss": 2.3993,
      "step": 20
    },
    {
      "epoch": 0.005726283641916396,
      "grad_norm": 43.393226623535156,
      "learning_rate": 0.00019984671392987165,
      "loss": 3.1412,
      "step": 30
    },
    {
      "epoch": 0.007635044855888528,
      "grad_norm": 37.4011344909668,
      "learning_rate": 0.0001994634987545507,
      "loss": 2.6345,
      "step": 40
    },
    {
      "epoch": 0.00954380606986066,
      "grad_norm": 79.2786865234375,
      "learning_rate": 0.00019908028357922973,
      "loss": 3.2536,
      "step": 50
    },
    {
      "epoch": 0.011452567283832793,
      "grad_norm": 53.92472839355469,
      "learning_rate": 0.0001986970684039088,
      "loss": 2.7537,
      "step": 60
    },
    {
      "epoch": 0.013361328497804924,
      "grad_norm": 71.84658813476562,
      "learning_rate": 0.00019831385322858786,
      "loss": 2.7005,
      "step": 70
    },
    {
      "epoch": 0.015270089711777056,
      "grad_norm": 56.58669662475586,
      "learning_rate": 0.00019793063805326693,
      "loss": 2.1771,
      "step": 80
    },
    {
      "epoch": 0.017178850925749188,
      "grad_norm": 71.64788818359375,
      "learning_rate": 0.000197547422877946,
      "loss": 2.1232,
      "step": 90
    },
    {
      "epoch": 0.01908761213972132,
      "grad_norm": 132.48135375976562,
      "learning_rate": 0.00019716420770262504,
      "loss": 2.9491,
      "step": 100
    },
    {
      "epoch": 0.02099637335369345,
      "grad_norm": 31.60591697692871,
      "learning_rate": 0.00019678099252730407,
      "loss": 2.8411,
      "step": 110
    },
    {
      "epoch": 0.022905134567665585,
      "grad_norm": 26.168376922607422,
      "learning_rate": 0.00019639777735198314,
      "loss": 2.7335,
      "step": 120
    },
    {
      "epoch": 0.02481389578163772,
      "grad_norm": 43.09207534790039,
      "learning_rate": 0.0001960145621766622,
      "loss": 2.0217,
      "step": 130
    },
    {
      "epoch": 0.02672265699560985,
      "grad_norm": 43.096336364746094,
      "learning_rate": 0.00019563134700134127,
      "loss": 2.5234,
      "step": 140
    },
    {
      "epoch": 0.028631418209581982,
      "grad_norm": 65.80716705322266,
      "learning_rate": 0.00019524813182602034,
      "loss": 1.9343,
      "step": 150
    },
    {
      "epoch": 0.030540179423554113,
      "grad_norm": 113.6764907836914,
      "learning_rate": 0.00019486491665069938,
      "loss": 2.9312,
      "step": 160
    },
    {
      "epoch": 0.03244894063752624,
      "grad_norm": 24.618350982666016,
      "learning_rate": 0.00019448170147537842,
      "loss": 1.9259,
      "step": 170
    },
    {
      "epoch": 0.034357701851498376,
      "grad_norm": 16.37238121032715,
      "learning_rate": 0.00019409848630005748,
      "loss": 2.1393,
      "step": 180
    },
    {
      "epoch": 0.03626646306547051,
      "grad_norm": 26.453733444213867,
      "learning_rate": 0.00019371527112473655,
      "loss": 2.3722,
      "step": 190
    },
    {
      "epoch": 0.03817522427944264,
      "grad_norm": 62.976539611816406,
      "learning_rate": 0.00019333205594941562,
      "loss": 2.3915,
      "step": 200
    },
    {
      "epoch": 0.04008398549341478,
      "grad_norm": 55.77000427246094,
      "learning_rate": 0.00019294884077409466,
      "loss": 2.0134,
      "step": 210
    },
    {
      "epoch": 0.0419927467073869,
      "grad_norm": 48.363895416259766,
      "learning_rate": 0.00019256562559877372,
      "loss": 2.7125,
      "step": 220
    },
    {
      "epoch": 0.04390150792135904,
      "grad_norm": 47.23270797729492,
      "learning_rate": 0.00019218241042345276,
      "loss": 2.2547,
      "step": 230
    },
    {
      "epoch": 0.04581026913533117,
      "grad_norm": 41.07670593261719,
      "learning_rate": 0.00019179919524813183,
      "loss": 2.0367,
      "step": 240
    },
    {
      "epoch": 0.047719030349303304,
      "grad_norm": 24.644947052001953,
      "learning_rate": 0.0001914159800728109,
      "loss": 1.8316,
      "step": 250
    },
    {
      "epoch": 0.04962779156327544,
      "grad_norm": 56.71709442138672,
      "learning_rate": 0.00019103276489748996,
      "loss": 2.7558,
      "step": 260
    },
    {
      "epoch": 0.051536552777247564,
      "grad_norm": 55.523826599121094,
      "learning_rate": 0.000190649549722169,
      "loss": 2.4394,
      "step": 270
    },
    {
      "epoch": 0.0534453139912197,
      "grad_norm": 14.039749145507812,
      "learning_rate": 0.00019026633454684807,
      "loss": 2.5673,
      "step": 280
    },
    {
      "epoch": 0.05535407520519183,
      "grad_norm": 66.36923217773438,
      "learning_rate": 0.0001898831193715271,
      "loss": 2.2043,
      "step": 290
    },
    {
      "epoch": 0.057262836419163965,
      "grad_norm": 55.96482467651367,
      "learning_rate": 0.00018949990419620617,
      "loss": 1.7759,
      "step": 300
    },
    {
      "epoch": 0.05917159763313609,
      "grad_norm": 22.85715103149414,
      "learning_rate": 0.00018911668902088524,
      "loss": 2.046,
      "step": 310
    },
    {
      "epoch": 0.061080358847108225,
      "grad_norm": 26.703815460205078,
      "learning_rate": 0.0001887334738455643,
      "loss": 2.4781,
      "step": 320
    },
    {
      "epoch": 0.06298912006108036,
      "grad_norm": 17.312570571899414,
      "learning_rate": 0.00018835025867024334,
      "loss": 2.2317,
      "step": 330
    },
    {
      "epoch": 0.06489788127505249,
      "grad_norm": 39.42776870727539,
      "learning_rate": 0.0001879670434949224,
      "loss": 2.0771,
      "step": 340
    },
    {
      "epoch": 0.06680664248902463,
      "grad_norm": 59.926231384277344,
      "learning_rate": 0.00018758382831960145,
      "loss": 2.0271,
      "step": 350
    },
    {
      "epoch": 0.06871540370299675,
      "grad_norm": 61.47703170776367,
      "learning_rate": 0.00018720061314428052,
      "loss": 2.5214,
      "step": 360
    },
    {
      "epoch": 0.07062416491696889,
      "grad_norm": 42.511898040771484,
      "learning_rate": 0.00018681739796895958,
      "loss": 2.6496,
      "step": 370
    },
    {
      "epoch": 0.07253292613094102,
      "grad_norm": 31.021581649780273,
      "learning_rate": 0.00018643418279363865,
      "loss": 1.9663,
      "step": 380
    },
    {
      "epoch": 0.07444168734491315,
      "grad_norm": 24.21485710144043,
      "learning_rate": 0.0001860509676183177,
      "loss": 2.5845,
      "step": 390
    },
    {
      "epoch": 0.07635044855888529,
      "grad_norm": 48.35919952392578,
      "learning_rate": 0.00018566775244299675,
      "loss": 3.2541,
      "step": 400
    },
    {
      "epoch": 0.07825920977285741,
      "grad_norm": 31.023876190185547,
      "learning_rate": 0.0001852845372676758,
      "loss": 1.7563,
      "step": 410
    },
    {
      "epoch": 0.08016797098682955,
      "grad_norm": 100.32941436767578,
      "learning_rate": 0.00018490132209235486,
      "loss": 2.3559,
      "step": 420
    },
    {
      "epoch": 0.08207673220080168,
      "grad_norm": 29.45631217956543,
      "learning_rate": 0.00018451810691703393,
      "loss": 1.8593,
      "step": 430
    },
    {
      "epoch": 0.0839854934147738,
      "grad_norm": 25.639190673828125,
      "learning_rate": 0.000184134891741713,
      "loss": 2.0414,
      "step": 440
    },
    {
      "epoch": 0.08589425462874595,
      "grad_norm": 20.370960235595703,
      "learning_rate": 0.00018375167656639203,
      "loss": 2.277,
      "step": 450
    },
    {
      "epoch": 0.08780301584271807,
      "grad_norm": 19.97625732421875,
      "learning_rate": 0.0001833684613910711,
      "loss": 2.3113,
      "step": 460
    },
    {
      "epoch": 0.08971177705669021,
      "grad_norm": 34.4874267578125,
      "learning_rate": 0.00018298524621575014,
      "loss": 2.1239,
      "step": 470
    },
    {
      "epoch": 0.09162053827066234,
      "grad_norm": 27.53536033630371,
      "learning_rate": 0.0001826020310404292,
      "loss": 1.89,
      "step": 480
    },
    {
      "epoch": 0.09352929948463447,
      "grad_norm": 36.64655303955078,
      "learning_rate": 0.00018221881586510827,
      "loss": 1.9285,
      "step": 490
    },
    {
      "epoch": 0.09543806069860661,
      "grad_norm": 73.37899780273438,
      "learning_rate": 0.00018183560068978734,
      "loss": 2.7601,
      "step": 500
    },
    {
      "epoch": 0.09734682191257873,
      "grad_norm": 31.71428108215332,
      "learning_rate": 0.00018145238551446638,
      "loss": 1.805,
      "step": 510
    },
    {
      "epoch": 0.09925558312655088,
      "grad_norm": 27.117708206176758,
      "learning_rate": 0.00018106917033914544,
      "loss": 2.1593,
      "step": 520
    },
    {
      "epoch": 0.101164344340523,
      "grad_norm": 48.132057189941406,
      "learning_rate": 0.00018068595516382448,
      "loss": 1.8715,
      "step": 530
    },
    {
      "epoch": 0.10307310555449513,
      "grad_norm": 71.60144805908203,
      "learning_rate": 0.00018030273998850355,
      "loss": 2.6071,
      "step": 540
    },
    {
      "epoch": 0.10498186676846727,
      "grad_norm": 28.03699493408203,
      "learning_rate": 0.00017991952481318261,
      "loss": 1.986,
      "step": 550
    },
    {
      "epoch": 0.1068906279824394,
      "grad_norm": 30.058837890625,
      "learning_rate": 0.00017953630963786168,
      "loss": 1.6868,
      "step": 560
    },
    {
      "epoch": 0.10879938919641152,
      "grad_norm": 12.182534217834473,
      "learning_rate": 0.00017919141598007283,
      "loss": 1.7223,
      "step": 570
    },
    {
      "epoch": 0.11070815041038366,
      "grad_norm": 43.05899429321289,
      "learning_rate": 0.00017880820080475187,
      "loss": 2.295,
      "step": 580
    },
    {
      "epoch": 0.11261691162435579,
      "grad_norm": 22.707801818847656,
      "learning_rate": 0.00017842498562943093,
      "loss": 2.3241,
      "step": 590
    },
    {
      "epoch": 0.11452567283832793,
      "grad_norm": 37.031402587890625,
      "learning_rate": 0.00017804177045411,
      "loss": 2.2825,
      "step": 600
    },
    {
      "epoch": 0.11643443405230006,
      "grad_norm": 12.065681457519531,
      "learning_rate": 0.00017765855527878904,
      "loss": 2.0715,
      "step": 610
    },
    {
      "epoch": 0.11834319526627218,
      "grad_norm": 40.33902359008789,
      "learning_rate": 0.0001772753401034681,
      "loss": 1.9413,
      "step": 620
    },
    {
      "epoch": 0.12025195648024432,
      "grad_norm": 29.57735252380371,
      "learning_rate": 0.00017689212492814717,
      "loss": 1.808,
      "step": 630
    },
    {
      "epoch": 0.12216071769421645,
      "grad_norm": 47.893280029296875,
      "learning_rate": 0.0001765089097528262,
      "loss": 1.8862,
      "step": 640
    },
    {
      "epoch": 0.12406947890818859,
      "grad_norm": 27.622007369995117,
      "learning_rate": 0.00017612569457750528,
      "loss": 2.0654,
      "step": 650
    },
    {
      "epoch": 0.12597824012216072,
      "grad_norm": 20.99016571044922,
      "learning_rate": 0.00017574247940218434,
      "loss": 2.4211,
      "step": 660
    },
    {
      "epoch": 0.12788700133613284,
      "grad_norm": 22.831398010253906,
      "learning_rate": 0.00017535926422686338,
      "loss": 2.2056,
      "step": 670
    },
    {
      "epoch": 0.12979576255010497,
      "grad_norm": 23.610673904418945,
      "learning_rate": 0.00017497604905154245,
      "loss": 1.7661,
      "step": 680
    },
    {
      "epoch": 0.13170452376407712,
      "grad_norm": 33.90515899658203,
      "learning_rate": 0.00017459283387622152,
      "loss": 1.7296,
      "step": 690
    },
    {
      "epoch": 0.13361328497804925,
      "grad_norm": 77.79330444335938,
      "learning_rate": 0.00017420961870090056,
      "loss": 2.324,
      "step": 700
    },
    {
      "epoch": 0.13552204619202138,
      "grad_norm": 27.26388931274414,
      "learning_rate": 0.00017382640352557962,
      "loss": 2.3764,
      "step": 710
    },
    {
      "epoch": 0.1374308074059935,
      "grad_norm": 28.484601974487305,
      "learning_rate": 0.0001734431883502587,
      "loss": 2.1678,
      "step": 720
    },
    {
      "epoch": 0.13933956861996563,
      "grad_norm": 43.24355697631836,
      "learning_rate": 0.00017305997317493773,
      "loss": 2.6092,
      "step": 730
    },
    {
      "epoch": 0.14124832983393779,
      "grad_norm": 38.31722640991211,
      "learning_rate": 0.0001726767579996168,
      "loss": 1.7078,
      "step": 740
    },
    {
      "epoch": 0.1431570910479099,
      "grad_norm": 21.55414390563965,
      "learning_rate": 0.00017229354282429583,
      "loss": 1.7344,
      "step": 750
    },
    {
      "epoch": 0.14506585226188204,
      "grad_norm": 32.68125534057617,
      "learning_rate": 0.0001719103276489749,
      "loss": 1.8825,
      "step": 760
    },
    {
      "epoch": 0.14697461347585417,
      "grad_norm": 35.879337310791016,
      "learning_rate": 0.00017152711247365397,
      "loss": 1.9778,
      "step": 770
    },
    {
      "epoch": 0.1488833746898263,
      "grad_norm": 50.45831298828125,
      "learning_rate": 0.00017114389729833303,
      "loss": 1.622,
      "step": 780
    },
    {
      "epoch": 0.15079213590379845,
      "grad_norm": 10.909369468688965,
      "learning_rate": 0.00017076068212301207,
      "loss": 1.3899,
      "step": 790
    },
    {
      "epoch": 0.15270089711777057,
      "grad_norm": 55.93587112426758,
      "learning_rate": 0.00017037746694769114,
      "loss": 2.6198,
      "step": 800
    },
    {
      "epoch": 0.1546096583317427,
      "grad_norm": 69.36833953857422,
      "learning_rate": 0.00016999425177237018,
      "loss": 2.3447,
      "step": 810
    },
    {
      "epoch": 0.15651841954571483,
      "grad_norm": 17.669540405273438,
      "learning_rate": 0.00016961103659704924,
      "loss": 1.6996,
      "step": 820
    },
    {
      "epoch": 0.15842718075968695,
      "grad_norm": 35.33816909790039,
      "learning_rate": 0.0001692278214217283,
      "loss": 1.4504,
      "step": 830
    },
    {
      "epoch": 0.1603359419736591,
      "grad_norm": 17.05954933166504,
      "learning_rate": 0.00016884460624640738,
      "loss": 2.386,
      "step": 840
    },
    {
      "epoch": 0.16224470318763123,
      "grad_norm": 38.81966781616211,
      "learning_rate": 0.00016846139107108642,
      "loss": 2.3285,
      "step": 850
    },
    {
      "epoch": 0.16415346440160336,
      "grad_norm": 22.259048461914062,
      "learning_rate": 0.00016807817589576548,
      "loss": 2.3976,
      "step": 860
    },
    {
      "epoch": 0.1660622256155755,
      "grad_norm": 59.53786849975586,
      "learning_rate": 0.00016769496072044452,
      "loss": 1.7783,
      "step": 870
    },
    {
      "epoch": 0.1679709868295476,
      "grad_norm": 24.429790496826172,
      "learning_rate": 0.0001673117455451236,
      "loss": 1.7153,
      "step": 880
    },
    {
      "epoch": 0.16987974804351977,
      "grad_norm": 17.53110122680664,
      "learning_rate": 0.00016692853036980265,
      "loss": 1.7564,
      "step": 890
    },
    {
      "epoch": 0.1717885092574919,
      "grad_norm": 31.02328872680664,
      "learning_rate": 0.00016654531519448172,
      "loss": 2.0332,
      "step": 900
    },
    {
      "epoch": 0.17369727047146402,
      "grad_norm": 11.291159629821777,
      "learning_rate": 0.00016616210001916076,
      "loss": 1.9068,
      "step": 910
    },
    {
      "epoch": 0.17560603168543615,
      "grad_norm": 8.379352569580078,
      "learning_rate": 0.00016577888484383983,
      "loss": 2.6516,
      "step": 920
    },
    {
      "epoch": 0.17751479289940827,
      "grad_norm": 16.572860717773438,
      "learning_rate": 0.00016539566966851886,
      "loss": 1.262,
      "step": 930
    },
    {
      "epoch": 0.17942355411338043,
      "grad_norm": 22.849109649658203,
      "learning_rate": 0.00016501245449319793,
      "loss": 1.8175,
      "step": 940
    },
    {
      "epoch": 0.18133231532735256,
      "grad_norm": 29.901878356933594,
      "learning_rate": 0.000164629239317877,
      "loss": 2.0776,
      "step": 950
    },
    {
      "epoch": 0.18324107654132468,
      "grad_norm": 36.59938430786133,
      "learning_rate": 0.00016424602414255606,
      "loss": 1.8507,
      "step": 960
    },
    {
      "epoch": 0.1851498377552968,
      "grad_norm": 35.43906021118164,
      "learning_rate": 0.0001638628089672351,
      "loss": 1.8466,
      "step": 970
    },
    {
      "epoch": 0.18705859896926894,
      "grad_norm": 43.46939468383789,
      "learning_rate": 0.00016347959379191417,
      "loss": 2.2906,
      "step": 980
    },
    {
      "epoch": 0.1889673601832411,
      "grad_norm": 23.804935455322266,
      "learning_rate": 0.0001630963786165932,
      "loss": 1.9811,
      "step": 990
    },
    {
      "epoch": 0.19087612139721322,
      "grad_norm": 22.595333099365234,
      "learning_rate": 0.00016271316344127227,
      "loss": 2.1464,
      "step": 1000
    },
    {
      "epoch": 0.19278488261118534,
      "grad_norm": 16.29434585571289,
      "learning_rate": 0.00016232994826595134,
      "loss": 2.3171,
      "step": 1010
    },
    {
      "epoch": 0.19469364382515747,
      "grad_norm": 20.390676498413086,
      "learning_rate": 0.0001619467330906304,
      "loss": 1.7852,
      "step": 1020
    },
    {
      "epoch": 0.1966024050391296,
      "grad_norm": 21.543813705444336,
      "learning_rate": 0.00016156351791530945,
      "loss": 1.8453,
      "step": 1030
    },
    {
      "epoch": 0.19851116625310175,
      "grad_norm": 41.68569564819336,
      "learning_rate": 0.0001611803027399885,
      "loss": 2.0802,
      "step": 1040
    },
    {
      "epoch": 0.20041992746707388,
      "grad_norm": 16.369949340820312,
      "learning_rate": 0.00016079708756466755,
      "loss": 2.3677,
      "step": 1050
    },
    {
      "epoch": 0.202328688681046,
      "grad_norm": 59.32874298095703,
      "learning_rate": 0.00016041387238934662,
      "loss": 2.3074,
      "step": 1060
    },
    {
      "epoch": 0.20423744989501813,
      "grad_norm": 25.006885528564453,
      "learning_rate": 0.00016003065721402568,
      "loss": 1.6293,
      "step": 1070
    },
    {
      "epoch": 0.20614621110899026,
      "grad_norm": 33.11625289916992,
      "learning_rate": 0.00015964744203870475,
      "loss": 2.0887,
      "step": 1080
    },
    {
      "epoch": 0.20805497232296238,
      "grad_norm": 17.52043914794922,
      "learning_rate": 0.0001592642268633838,
      "loss": 1.6037,
      "step": 1090
    },
    {
      "epoch": 0.20996373353693454,
      "grad_norm": 62.3324089050293,
      "learning_rate": 0.00015888101168806286,
      "loss": 2.1705,
      "step": 1100
    },
    {
      "epoch": 0.21187249475090666,
      "grad_norm": 23.646345138549805,
      "learning_rate": 0.0001584977965127419,
      "loss": 2.5532,
      "step": 1110
    },
    {
      "epoch": 0.2137812559648788,
      "grad_norm": 48.99565124511719,
      "learning_rate": 0.00015811458133742096,
      "loss": 1.7953,
      "step": 1120
    },
    {
      "epoch": 0.21569001717885092,
      "grad_norm": 50.91920471191406,
      "learning_rate": 0.00015773136616210003,
      "loss": 1.5821,
      "step": 1130
    },
    {
      "epoch": 0.21759877839282304,
      "grad_norm": 19.882246017456055,
      "learning_rate": 0.0001573481509867791,
      "loss": 1.7491,
      "step": 1140
    },
    {
      "epoch": 0.2195075396067952,
      "grad_norm": 32.795127868652344,
      "learning_rate": 0.00015696493581145813,
      "loss": 1.8662,
      "step": 1150
    },
    {
      "epoch": 0.22141630082076733,
      "grad_norm": 19.980140686035156,
      "learning_rate": 0.0001565817206361372,
      "loss": 1.7875,
      "step": 1160
    },
    {
      "epoch": 0.22332506203473945,
      "grad_norm": 42.100929260253906,
      "learning_rate": 0.00015619850546081624,
      "loss": 2.08,
      "step": 1170
    },
    {
      "epoch": 0.22523382324871158,
      "grad_norm": 39.585304260253906,
      "learning_rate": 0.0001558152902854953,
      "loss": 1.7834,
      "step": 1180
    },
    {
      "epoch": 0.2271425844626837,
      "grad_norm": 30.439823150634766,
      "learning_rate": 0.00015543207511017437,
      "loss": 1.8953,
      "step": 1190
    },
    {
      "epoch": 0.22905134567665586,
      "grad_norm": 33.56026077270508,
      "learning_rate": 0.00015504885993485344,
      "loss": 2.1024,
      "step": 1200
    },
    {
      "epoch": 0.230960106890628,
      "grad_norm": 24.36527442932129,
      "learning_rate": 0.00015466564475953248,
      "loss": 1.6907,
      "step": 1210
    },
    {
      "epoch": 0.2328688681046001,
      "grad_norm": 42.80393981933594,
      "learning_rate": 0.00015428242958421154,
      "loss": 1.8982,
      "step": 1220
    },
    {
      "epoch": 0.23477762931857224,
      "grad_norm": 36.27048873901367,
      "learning_rate": 0.00015389921440889058,
      "loss": 2.3072,
      "step": 1230
    },
    {
      "epoch": 0.23668639053254437,
      "grad_norm": 14.763938903808594,
      "learning_rate": 0.00015351599923356965,
      "loss": 1.7763,
      "step": 1240
    },
    {
      "epoch": 0.23859515174651652,
      "grad_norm": 53.93087387084961,
      "learning_rate": 0.00015313278405824872,
      "loss": 2.14,
      "step": 1250
    },
    {
      "epoch": 0.24050391296048865,
      "grad_norm": 16.601242065429688,
      "learning_rate": 0.00015274956888292778,
      "loss": 1.9224,
      "step": 1260
    },
    {
      "epoch": 0.24241267417446077,
      "grad_norm": 29.895851135253906,
      "learning_rate": 0.00015236635370760682,
      "loss": 1.6012,
      "step": 1270
    },
    {
      "epoch": 0.2443214353884329,
      "grad_norm": 14.211260795593262,
      "learning_rate": 0.0001519831385322859,
      "loss": 2.0299,
      "step": 1280
    },
    {
      "epoch": 0.24623019660240503,
      "grad_norm": 32.839847564697266,
      "learning_rate": 0.00015159992335696493,
      "loss": 2.6818,
      "step": 1290
    },
    {
      "epoch": 0.24813895781637718,
      "grad_norm": 55.2479133605957,
      "learning_rate": 0.000151216708181644,
      "loss": 2.0284,
      "step": 1300
    },
    {
      "epoch": 0.2500477190303493,
      "grad_norm": 60.6522331237793,
      "learning_rate": 0.00015083349300632306,
      "loss": 2.3021,
      "step": 1310
    },
    {
      "epoch": 0.25195648024432143,
      "grad_norm": 42.86838150024414,
      "learning_rate": 0.00015045027783100213,
      "loss": 1.9095,
      "step": 1320
    },
    {
      "epoch": 0.2538652414582936,
      "grad_norm": 14.39518928527832,
      "learning_rate": 0.0001500670626556812,
      "loss": 1.661,
      "step": 1330
    },
    {
      "epoch": 0.2557740026722657,
      "grad_norm": 9.865754127502441,
      "learning_rate": 0.00014968384748036023,
      "loss": 1.1735,
      "step": 1340
    },
    {
      "epoch": 0.25768276388623784,
      "grad_norm": 38.86551284790039,
      "learning_rate": 0.00014930063230503927,
      "loss": 2.2301,
      "step": 1350
    },
    {
      "epoch": 0.25959152510020994,
      "grad_norm": 25.97415542602539,
      "learning_rate": 0.00014891741712971834,
      "loss": 1.713,
      "step": 1360
    },
    {
      "epoch": 0.2615002863141821,
      "grad_norm": 33.62382125854492,
      "learning_rate": 0.0001485342019543974,
      "loss": 1.9324,
      "step": 1370
    },
    {
      "epoch": 0.26340904752815425,
      "grad_norm": 52.55062484741211,
      "learning_rate": 0.00014815098677907647,
      "loss": 1.743,
      "step": 1380
    },
    {
      "epoch": 0.26531780874212635,
      "grad_norm": 40.716304779052734,
      "learning_rate": 0.00014776777160375554,
      "loss": 2.0119,
      "step": 1390
    },
    {
      "epoch": 0.2672265699560985,
      "grad_norm": 18.68084144592285,
      "learning_rate": 0.00014738455642843458,
      "loss": 2.0909,
      "step": 1400
    },
    {
      "epoch": 0.2691353311700706,
      "grad_norm": 33.41558074951172,
      "learning_rate": 0.00014700134125311361,
      "loss": 1.898,
      "step": 1410
    },
    {
      "epoch": 0.27104409238404276,
      "grad_norm": 14.323453903198242,
      "learning_rate": 0.00014661812607779268,
      "loss": 1.8827,
      "step": 1420
    },
    {
      "epoch": 0.2729528535980149,
      "grad_norm": 13.97889518737793,
      "learning_rate": 0.00014623491090247175,
      "loss": 1.8921,
      "step": 1430
    },
    {
      "epoch": 0.274861614811987,
      "grad_norm": 53.84613800048828,
      "learning_rate": 0.0001458516957271508,
      "loss": 2.0396,
      "step": 1440
    },
    {
      "epoch": 0.27677037602595916,
      "grad_norm": 30.348552703857422,
      "learning_rate": 0.00014546848055182988,
      "loss": 2.2559,
      "step": 1450
    },
    {
      "epoch": 0.27867913723993126,
      "grad_norm": 18.113466262817383,
      "learning_rate": 0.00014508526537650892,
      "loss": 1.9302,
      "step": 1460
    },
    {
      "epoch": 0.2805878984539034,
      "grad_norm": 28.333881378173828,
      "learning_rate": 0.00014470205020118796,
      "loss": 2.0359,
      "step": 1470
    },
    {
      "epoch": 0.28249665966787557,
      "grad_norm": 29.156356811523438,
      "learning_rate": 0.00014431883502586702,
      "loss": 1.4034,
      "step": 1480
    },
    {
      "epoch": 0.28440542088184767,
      "grad_norm": 47.77531433105469,
      "learning_rate": 0.0001439356198505461,
      "loss": 2.2641,
      "step": 1490
    },
    {
      "epoch": 0.2863141820958198,
      "grad_norm": 35.64164733886719,
      "learning_rate": 0.00014355240467522516,
      "loss": 1.9678,
      "step": 1500
    },
    {
      "epoch": 0.2882229433097919,
      "grad_norm": 36.61343765258789,
      "learning_rate": 0.00014316918949990422,
      "loss": 1.8503,
      "step": 1510
    },
    {
      "epoch": 0.2901317045237641,
      "grad_norm": 25.74701499938965,
      "learning_rate": 0.00014278597432458326,
      "loss": 1.5846,
      "step": 1520
    },
    {
      "epoch": 0.29204046573773623,
      "grad_norm": 66.93306732177734,
      "learning_rate": 0.0001424027591492623,
      "loss": 2.096,
      "step": 1530
    },
    {
      "epoch": 0.29394922695170833,
      "grad_norm": 36.27447509765625,
      "learning_rate": 0.00014201954397394137,
      "loss": 2.4632,
      "step": 1540
    },
    {
      "epoch": 0.2958579881656805,
      "grad_norm": 27.35124397277832,
      "learning_rate": 0.00014163632879862043,
      "loss": 2.1863,
      "step": 1550
    },
    {
      "epoch": 0.2977667493796526,
      "grad_norm": 9.608837127685547,
      "learning_rate": 0.0001412531136232995,
      "loss": 1.3354,
      "step": 1560
    },
    {
      "epoch": 0.29967551059362474,
      "grad_norm": 20.517574310302734,
      "learning_rate": 0.00014086989844797857,
      "loss": 1.8995,
      "step": 1570
    },
    {
      "epoch": 0.3015842718075969,
      "grad_norm": 25.83588409423828,
      "learning_rate": 0.0001404866832726576,
      "loss": 1.852,
      "step": 1580
    },
    {
      "epoch": 0.303493033021569,
      "grad_norm": 25.699058532714844,
      "learning_rate": 0.00014010346809733665,
      "loss": 2.3005,
      "step": 1590
    },
    {
      "epoch": 0.30540179423554115,
      "grad_norm": 48.62494659423828,
      "learning_rate": 0.0001397202529220157,
      "loss": 1.3764,
      "step": 1600
    },
    {
      "epoch": 0.30731055544951325,
      "grad_norm": 33.94823455810547,
      "learning_rate": 0.00013933703774669478,
      "loss": 1.7128,
      "step": 1610
    },
    {
      "epoch": 0.3092193166634854,
      "grad_norm": 31.997974395751953,
      "learning_rate": 0.00013895382257137384,
      "loss": 2.2602,
      "step": 1620
    },
    {
      "epoch": 0.31112807787745755,
      "grad_norm": 28.630840301513672,
      "learning_rate": 0.0001385706073960529,
      "loss": 1.9381,
      "step": 1630
    },
    {
      "epoch": 0.31303683909142965,
      "grad_norm": 33.946929931640625,
      "learning_rate": 0.00013818739222073195,
      "loss": 1.9617,
      "step": 1640
    },
    {
      "epoch": 0.3149456003054018,
      "grad_norm": 51.29664993286133,
      "learning_rate": 0.000137804177045411,
      "loss": 2.1509,
      "step": 1650
    },
    {
      "epoch": 0.3168543615193739,
      "grad_norm": 31.47586441040039,
      "learning_rate": 0.00013742096187009006,
      "loss": 1.6512,
      "step": 1660
    },
    {
      "epoch": 0.31876312273334606,
      "grad_norm": 26.953250885009766,
      "learning_rate": 0.00013703774669476912,
      "loss": 1.9008,
      "step": 1670
    },
    {
      "epoch": 0.3206718839473182,
      "grad_norm": 28.97018051147461,
      "learning_rate": 0.0001366545315194482,
      "loss": 2.8059,
      "step": 1680
    },
    {
      "epoch": 0.3225806451612903,
      "grad_norm": 46.57340621948242,
      "learning_rate": 0.00013627131634412725,
      "loss": 1.9583,
      "step": 1690
    },
    {
      "epoch": 0.32448940637526247,
      "grad_norm": 30.608856201171875,
      "learning_rate": 0.0001358881011688063,
      "loss": 1.6906,
      "step": 1700
    },
    {
      "epoch": 0.32639816758923457,
      "grad_norm": 11.621082305908203,
      "learning_rate": 0.00013550488599348533,
      "loss": 1.7208,
      "step": 1710
    },
    {
      "epoch": 0.3283069288032067,
      "grad_norm": 65.46932983398438,
      "learning_rate": 0.0001351216708181644,
      "loss": 2.0177,
      "step": 1720
    },
    {
      "epoch": 0.3302156900171789,
      "grad_norm": 24.651338577270508,
      "learning_rate": 0.00013473845564284347,
      "loss": 1.9901,
      "step": 1730
    },
    {
      "epoch": 0.332124451231151,
      "grad_norm": 42.008750915527344,
      "learning_rate": 0.00013435524046752253,
      "loss": 1.7957,
      "step": 1740
    },
    {
      "epoch": 0.33403321244512313,
      "grad_norm": 23.107608795166016,
      "learning_rate": 0.0001339720252922016,
      "loss": 1.6542,
      "step": 1750
    },
    {
      "epoch": 0.3359419736590952,
      "grad_norm": 40.422122955322266,
      "learning_rate": 0.00013358881011688064,
      "loss": 2.2597,
      "step": 1760
    },
    {
      "epoch": 0.3378507348730674,
      "grad_norm": 23.10890769958496,
      "learning_rate": 0.00013320559494155968,
      "loss": 1.511,
      "step": 1770
    },
    {
      "epoch": 0.33975949608703954,
      "grad_norm": 28.641965866088867,
      "learning_rate": 0.00013282237976623874,
      "loss": 2.5149,
      "step": 1780
    },
    {
      "epoch": 0.34166825730101164,
      "grad_norm": 26.386842727661133,
      "learning_rate": 0.0001324391645909178,
      "loss": 1.5377,
      "step": 1790
    },
    {
      "epoch": 0.3435770185149838,
      "grad_norm": 16.91390037536621,
      "learning_rate": 0.00013205594941559688,
      "loss": 1.8703,
      "step": 1800
    },
    {
      "epoch": 0.3454857797289559,
      "grad_norm": 10.470297813415527,
      "learning_rate": 0.00013167273424027594,
      "loss": 2.3852,
      "step": 1810
    },
    {
      "epoch": 0.34739454094292804,
      "grad_norm": 49.14983367919922,
      "learning_rate": 0.00013128951906495498,
      "loss": 1.8319,
      "step": 1820
    },
    {
      "epoch": 0.3493033021569002,
      "grad_norm": 21.844331741333008,
      "learning_rate": 0.00013090630388963402,
      "loss": 1.9063,
      "step": 1830
    },
    {
      "epoch": 0.3512120633708723,
      "grad_norm": 23.250186920166016,
      "learning_rate": 0.0001305230887143131,
      "loss": 1.8532,
      "step": 1840
    },
    {
      "epoch": 0.35312082458484445,
      "grad_norm": 24.773828506469727,
      "learning_rate": 0.00013013987353899215,
      "loss": 1.9539,
      "step": 1850
    },
    {
      "epoch": 0.35502958579881655,
      "grad_norm": 14.003202438354492,
      "learning_rate": 0.00012975665836367122,
      "loss": 2.0748,
      "step": 1860
    },
    {
      "epoch": 0.3569383470127887,
      "grad_norm": 29.572778701782227,
      "learning_rate": 0.00012937344318835029,
      "loss": 1.749,
      "step": 1870
    },
    {
      "epoch": 0.35884710822676086,
      "grad_norm": 42.92353057861328,
      "learning_rate": 0.00012899022801302933,
      "loss": 1.6342,
      "step": 1880
    },
    {
      "epoch": 0.36075586944073296,
      "grad_norm": 36.934295654296875,
      "learning_rate": 0.00012860701283770836,
      "loss": 1.9285,
      "step": 1890
    },
    {
      "epoch": 0.3626646306547051,
      "grad_norm": 24.259750366210938,
      "learning_rate": 0.00012822379766238743,
      "loss": 1.6346,
      "step": 1900
    },
    {
      "epoch": 0.3645733918686772,
      "grad_norm": 22.231224060058594,
      "learning_rate": 0.0001278405824870665,
      "loss": 1.8163,
      "step": 1910
    },
    {
      "epoch": 0.36648215308264936,
      "grad_norm": 22.428789138793945,
      "learning_rate": 0.00012745736731174556,
      "loss": 1.9967,
      "step": 1920
    },
    {
      "epoch": 0.3683909142966215,
      "grad_norm": 34.04105758666992,
      "learning_rate": 0.00012707415213642463,
      "loss": 1.777,
      "step": 1930
    },
    {
      "epoch": 0.3702996755105936,
      "grad_norm": 29.966907501220703,
      "learning_rate": 0.00012669093696110367,
      "loss": 2.1697,
      "step": 1940
    },
    {
      "epoch": 0.37220843672456577,
      "grad_norm": 55.59141540527344,
      "learning_rate": 0.0001263077217857827,
      "loss": 1.7195,
      "step": 1950
    },
    {
      "epoch": 0.37411719793853787,
      "grad_norm": 33.14993667602539,
      "learning_rate": 0.00012592450661046177,
      "loss": 1.6633,
      "step": 1960
    },
    {
      "epoch": 0.37602595915251,
      "grad_norm": 40.15737533569336,
      "learning_rate": 0.00012554129143514084,
      "loss": 2.2983,
      "step": 1970
    },
    {
      "epoch": 0.3779347203664822,
      "grad_norm": 40.69730758666992,
      "learning_rate": 0.0001251580762598199,
      "loss": 1.6397,
      "step": 1980
    },
    {
      "epoch": 0.3798434815804543,
      "grad_norm": 40.146915435791016,
      "learning_rate": 0.00012477486108449897,
      "loss": 1.8756,
      "step": 1990
    },
    {
      "epoch": 0.38175224279442643,
      "grad_norm": 11.77124309539795,
      "learning_rate": 0.000124391645909178,
      "loss": 1.3694,
      "step": 2000
    },
    {
      "epoch": 0.38366100400839853,
      "grad_norm": 23.51131820678711,
      "learning_rate": 0.00012400843073385705,
      "loss": 2.0371,
      "step": 2010
    },
    {
      "epoch": 0.3855697652223707,
      "grad_norm": 15.009212493896484,
      "learning_rate": 0.00012362521555853612,
      "loss": 2.0603,
      "step": 2020
    },
    {
      "epoch": 0.38747852643634284,
      "grad_norm": 39.0299186706543,
      "learning_rate": 0.00012324200038321518,
      "loss": 1.8986,
      "step": 2030
    },
    {
      "epoch": 0.38938728765031494,
      "grad_norm": 24.145872116088867,
      "learning_rate": 0.00012285878520789425,
      "loss": 1.8178,
      "step": 2040
    },
    {
      "epoch": 0.3912960488642871,
      "grad_norm": 6.8277130126953125,
      "learning_rate": 0.00012247557003257332,
      "loss": 1.7561,
      "step": 2050
    },
    {
      "epoch": 0.3932048100782592,
      "grad_norm": 47.5086555480957,
      "learning_rate": 0.00012209235485725236,
      "loss": 1.9289,
      "step": 2060
    },
    {
      "epoch": 0.39511357129223135,
      "grad_norm": 17.56279182434082,
      "learning_rate": 0.00012170913968193141,
      "loss": 2.486,
      "step": 2070
    },
    {
      "epoch": 0.3970223325062035,
      "grad_norm": 38.59670639038086,
      "learning_rate": 0.00012132592450661046,
      "loss": 1.3334,
      "step": 2080
    },
    {
      "epoch": 0.3989310937201756,
      "grad_norm": 21.769617080688477,
      "learning_rate": 0.00012094270933128953,
      "loss": 2.1422,
      "step": 2090
    },
    {
      "epoch": 0.40083985493414775,
      "grad_norm": 43.487918853759766,
      "learning_rate": 0.0001205594941559686,
      "loss": 1.9274,
      "step": 2100
    },
    {
      "epoch": 0.40274861614811985,
      "grad_norm": 31.48927879333496,
      "learning_rate": 0.00012017627898064765,
      "loss": 2.0317,
      "step": 2110
    },
    {
      "epoch": 0.404657377362092,
      "grad_norm": 51.33644485473633,
      "learning_rate": 0.00011979306380532669,
      "loss": 2.2476,
      "step": 2120
    },
    {
      "epoch": 0.4065661385760641,
      "grad_norm": 27.341575622558594,
      "learning_rate": 0.00011940984863000575,
      "loss": 1.6543,
      "step": 2130
    },
    {
      "epoch": 0.40847489979003626,
      "grad_norm": 46.32181930541992,
      "learning_rate": 0.0001190266334546848,
      "loss": 2.1249,
      "step": 2140
    },
    {
      "epoch": 0.4103836610040084,
      "grad_norm": 14.932916641235352,
      "learning_rate": 0.00011864341827936387,
      "loss": 1.9261,
      "step": 2150
    },
    {
      "epoch": 0.4122924222179805,
      "grad_norm": 13.490090370178223,
      "learning_rate": 0.00011826020310404294,
      "loss": 1.9038,
      "step": 2160
    },
    {
      "epoch": 0.41420118343195267,
      "grad_norm": 23.562833786010742,
      "learning_rate": 0.00011787698792872199,
      "loss": 1.6112,
      "step": 2170
    },
    {
      "epoch": 0.41610994464592477,
      "grad_norm": 28.634931564331055,
      "learning_rate": 0.00011749377275340103,
      "loss": 1.9421,
      "step": 2180
    },
    {
      "epoch": 0.4180187058598969,
      "grad_norm": 48.93767547607422,
      "learning_rate": 0.0001171105575780801,
      "loss": 1.8457,
      "step": 2190
    },
    {
      "epoch": 0.4199274670738691,
      "grad_norm": 27.789472579956055,
      "learning_rate": 0.00011672734240275915,
      "loss": 2.1161,
      "step": 2200
    },
    {
      "epoch": 0.4218362282878412,
      "grad_norm": 20.039033889770508,
      "learning_rate": 0.00011634412722743822,
      "loss": 1.853,
      "step": 2210
    },
    {
      "epoch": 0.42374498950181333,
      "grad_norm": 18.283370971679688,
      "learning_rate": 0.00011596091205211728,
      "loss": 1.5664,
      "step": 2220
    },
    {
      "epoch": 0.42565375071578543,
      "grad_norm": 17.941408157348633,
      "learning_rate": 0.00011557769687679634,
      "loss": 1.8868,
      "step": 2230
    },
    {
      "epoch": 0.4275625119297576,
      "grad_norm": 12.262140274047852,
      "learning_rate": 0.00011519448170147537,
      "loss": 1.8519,
      "step": 2240
    },
    {
      "epoch": 0.42947127314372974,
      "grad_norm": 31.767757415771484,
      "learning_rate": 0.00011481126652615444,
      "loss": 1.7354,
      "step": 2250
    },
    {
      "epoch": 0.43138003435770184,
      "grad_norm": 14.631132125854492,
      "learning_rate": 0.0001144280513508335,
      "loss": 1.8942,
      "step": 2260
    },
    {
      "epoch": 0.433288795571674,
      "grad_norm": 44.446102142333984,
      "learning_rate": 0.00011404483617551256,
      "loss": 1.9395,
      "step": 2270
    },
    {
      "epoch": 0.4351975567856461,
      "grad_norm": 13.768721580505371,
      "learning_rate": 0.00011366162100019163,
      "loss": 1.926,
      "step": 2280
    },
    {
      "epoch": 0.43710631799961824,
      "grad_norm": 58.7286376953125,
      "learning_rate": 0.00011327840582487067,
      "loss": 2.1639,
      "step": 2290
    },
    {
      "epoch": 0.4390150792135904,
      "grad_norm": 12.624917984008789,
      "learning_rate": 0.00011289519064954972,
      "loss": 1.7752,
      "step": 2300
    },
    {
      "epoch": 0.4409238404275625,
      "grad_norm": 8.861907005310059,
      "learning_rate": 0.00011251197547422878,
      "loss": 1.3457,
      "step": 2310
    },
    {
      "epoch": 0.44283260164153465,
      "grad_norm": 30.291736602783203,
      "learning_rate": 0.00011212876029890784,
      "loss": 2.0485,
      "step": 2320
    },
    {
      "epoch": 0.44474136285550675,
      "grad_norm": 44.758731842041016,
      "learning_rate": 0.0001117455451235869,
      "loss": 1.7903,
      "step": 2330
    },
    {
      "epoch": 0.4466501240694789,
      "grad_norm": 22.169374465942383,
      "learning_rate": 0.00011136232994826597,
      "loss": 1.7094,
      "step": 2340
    },
    {
      "epoch": 0.44855888528345106,
      "grad_norm": 35.010520935058594,
      "learning_rate": 0.00011097911477294501,
      "loss": 2.3943,
      "step": 2350
    },
    {
      "epoch": 0.45046764649742316,
      "grad_norm": 26.326244354248047,
      "learning_rate": 0.00011059589959762406,
      "loss": 1.8816,
      "step": 2360
    },
    {
      "epoch": 0.4523764077113953,
      "grad_norm": 17.010639190673828,
      "learning_rate": 0.00011021268442230313,
      "loss": 1.5555,
      "step": 2370
    },
    {
      "epoch": 0.4542851689253674,
      "grad_norm": 21.46563720703125,
      "learning_rate": 0.00010982946924698218,
      "loss": 1.591,
      "step": 2380
    },
    {
      "epoch": 0.45619393013933957,
      "grad_norm": 16.434955596923828,
      "learning_rate": 0.00010944625407166125,
      "loss": 2.6148,
      "step": 2390
    },
    {
      "epoch": 0.4581026913533117,
      "grad_norm": 20.789968490600586,
      "learning_rate": 0.00010906303889634031,
      "loss": 1.7655,
      "step": 2400
    },
    {
      "epoch": 0.4600114525672838,
      "grad_norm": 22.763818740844727,
      "learning_rate": 0.00010867982372101935,
      "loss": 2.0713,
      "step": 2410
    },
    {
      "epoch": 0.461920213781256,
      "grad_norm": 10.258102416992188,
      "learning_rate": 0.0001082966085456984,
      "loss": 1.9088,
      "step": 2420
    },
    {
      "epoch": 0.46382897499522807,
      "grad_norm": 35.63687515258789,
      "learning_rate": 0.00010791339337037747,
      "loss": 1.8379,
      "step": 2430
    },
    {
      "epoch": 0.4657377362092002,
      "grad_norm": 26.769319534301758,
      "learning_rate": 0.00010753017819505652,
      "loss": 1.9835,
      "step": 2440
    },
    {
      "epoch": 0.4676464974231724,
      "grad_norm": 28.311859130859375,
      "learning_rate": 0.00010714696301973559,
      "loss": 1.9259,
      "step": 2450
    },
    {
      "epoch": 0.4695552586371445,
      "grad_norm": 9.727643013000488,
      "learning_rate": 0.00010676374784441466,
      "loss": 2.1298,
      "step": 2460
    },
    {
      "epoch": 0.47146401985111663,
      "grad_norm": 23.833484649658203,
      "learning_rate": 0.0001063805326690937,
      "loss": 1.4682,
      "step": 2470
    },
    {
      "epoch": 0.47337278106508873,
      "grad_norm": 17.15323257446289,
      "learning_rate": 0.00010599731749377275,
      "loss": 1.4886,
      "step": 2480
    },
    {
      "epoch": 0.4752815422790609,
      "grad_norm": 22.561351776123047,
      "learning_rate": 0.00010561410231845182,
      "loss": 2.2559,
      "step": 2490
    },
    {
      "epoch": 0.47719030349303304,
      "grad_norm": 31.149478912353516,
      "learning_rate": 0.00010523088714313087,
      "loss": 1.8193,
      "step": 2500
    },
    {
      "epoch": 0.47909906470700514,
      "grad_norm": 38.318702697753906,
      "learning_rate": 0.00010484767196780993,
      "loss": 1.7183,
      "step": 2510
    },
    {
      "epoch": 0.4810078259209773,
      "grad_norm": 30.719562530517578,
      "learning_rate": 0.000104464456792489,
      "loss": 1.9197,
      "step": 2520
    },
    {
      "epoch": 0.4829165871349494,
      "grad_norm": 38.68711471557617,
      "learning_rate": 0.00010408124161716804,
      "loss": 1.6921,
      "step": 2530
    },
    {
      "epoch": 0.48482534834892155,
      "grad_norm": 16.942218780517578,
      "learning_rate": 0.0001036980264418471,
      "loss": 1.6935,
      "step": 2540
    },
    {
      "epoch": 0.4867341095628937,
      "grad_norm": 35.24391555786133,
      "learning_rate": 0.00010331481126652616,
      "loss": 1.7737,
      "step": 2550
    },
    {
      "epoch": 0.4886428707768658,
      "grad_norm": 30.261661529541016,
      "learning_rate": 0.00010293159609120521,
      "loss": 2.1336,
      "step": 2560
    },
    {
      "epoch": 0.49055163199083796,
      "grad_norm": 46.530487060546875,
      "learning_rate": 0.00010254838091588428,
      "loss": 2.0582,
      "step": 2570
    },
    {
      "epoch": 0.49246039320481005,
      "grad_norm": 32.5179328918457,
      "learning_rate": 0.00010216516574056335,
      "loss": 1.7889,
      "step": 2580
    },
    {
      "epoch": 0.4943691544187822,
      "grad_norm": 14.631001472473145,
      "learning_rate": 0.00010178195056524238,
      "loss": 1.5904,
      "step": 2590
    },
    {
      "epoch": 0.49627791563275436,
      "grad_norm": 22.84343147277832,
      "learning_rate": 0.00010139873538992144,
      "loss": 1.2136,
      "step": 2600
    },
    {
      "epoch": 0.49818667684672646,
      "grad_norm": 48.8205680847168,
      "learning_rate": 0.0001010155202146005,
      "loss": 1.8063,
      "step": 2610
    },
    {
      "epoch": 0.5000954380606986,
      "grad_norm": 35.58665084838867,
      "learning_rate": 0.00010063230503927956,
      "loss": 1.5221,
      "step": 2620
    },
    {
      "epoch": 0.5020041992746708,
      "grad_norm": 21.45016860961914,
      "learning_rate": 0.00010024908986395862,
      "loss": 1.7317,
      "step": 2630
    },
    {
      "epoch": 0.5039129604886429,
      "grad_norm": 19.265647888183594,
      "learning_rate": 9.986587468863768e-05,
      "loss": 1.142,
      "step": 2640
    },
    {
      "epoch": 0.505821721702615,
      "grad_norm": 12.599711418151855,
      "learning_rate": 9.948265951331673e-05,
      "loss": 1.7196,
      "step": 2650
    },
    {
      "epoch": 0.5077304829165872,
      "grad_norm": 36.9140510559082,
      "learning_rate": 9.90994443379958e-05,
      "loss": 1.7152,
      "step": 2660
    },
    {
      "epoch": 0.5096392441305593,
      "grad_norm": 12.49243450164795,
      "learning_rate": 9.871622916267485e-05,
      "loss": 1.6436,
      "step": 2670
    },
    {
      "epoch": 0.5115480053445314,
      "grad_norm": 15.071431159973145,
      "learning_rate": 9.83330139873539e-05,
      "loss": 1.6511,
      "step": 2680
    },
    {
      "epoch": 0.5134567665585036,
      "grad_norm": 33.78696060180664,
      "learning_rate": 9.794979881203295e-05,
      "loss": 1.4801,
      "step": 2690
    },
    {
      "epoch": 0.5153655277724757,
      "grad_norm": 23.124818801879883,
      "learning_rate": 9.756658363671202e-05,
      "loss": 2.0551,
      "step": 2700
    },
    {
      "epoch": 0.5172742889864478,
      "grad_norm": 20.99131202697754,
      "learning_rate": 9.718336846139107e-05,
      "loss": 1.9529,
      "step": 2710
    },
    {
      "epoch": 0.5191830502004199,
      "grad_norm": 23.091171264648438,
      "learning_rate": 9.680015328607012e-05,
      "loss": 1.9241,
      "step": 2720
    },
    {
      "epoch": 0.5210918114143921,
      "grad_norm": 30.51984977722168,
      "learning_rate": 9.641693811074919e-05,
      "loss": 1.9546,
      "step": 2730
    },
    {
      "epoch": 0.5230005726283642,
      "grad_norm": 13.598468780517578,
      "learning_rate": 9.603372293542824e-05,
      "loss": 1.6372,
      "step": 2740
    },
    {
      "epoch": 0.5249093338423363,
      "grad_norm": 28.83839225769043,
      "learning_rate": 9.56505077601073e-05,
      "loss": 2.0347,
      "step": 2750
    },
    {
      "epoch": 0.5268180950563085,
      "grad_norm": 40.47397232055664,
      "learning_rate": 9.526729258478636e-05,
      "loss": 1.8543,
      "step": 2760
    },
    {
      "epoch": 0.5287268562702806,
      "grad_norm": 17.771814346313477,
      "learning_rate": 9.488407740946542e-05,
      "loss": 1.625,
      "step": 2770
    },
    {
      "epoch": 0.5306356174842527,
      "grad_norm": 30.193126678466797,
      "learning_rate": 9.450086223414447e-05,
      "loss": 1.3894,
      "step": 2780
    },
    {
      "epoch": 0.5325443786982249,
      "grad_norm": 20.98810386657715,
      "learning_rate": 9.411764705882353e-05,
      "loss": 1.7972,
      "step": 2790
    },
    {
      "epoch": 0.534453139912197,
      "grad_norm": 35.050819396972656,
      "learning_rate": 9.373443188350259e-05,
      "loss": 1.9168,
      "step": 2800
    },
    {
      "epoch": 0.5363619011261691,
      "grad_norm": 37.35135269165039,
      "learning_rate": 9.335121670818164e-05,
      "loss": 1.8247,
      "step": 2810
    },
    {
      "epoch": 0.5382706623401412,
      "grad_norm": 17.18873405456543,
      "learning_rate": 9.29680015328607e-05,
      "loss": 1.3811,
      "step": 2820
    },
    {
      "epoch": 0.5401794235541134,
      "grad_norm": 22.853145599365234,
      "learning_rate": 9.258478635753976e-05,
      "loss": 1.8046,
      "step": 2830
    },
    {
      "epoch": 0.5420881847680855,
      "grad_norm": 26.13298988342285,
      "learning_rate": 9.220157118221881e-05,
      "loss": 1.7349,
      "step": 2840
    },
    {
      "epoch": 0.5439969459820576,
      "grad_norm": 50.4821662902832,
      "learning_rate": 9.181835600689788e-05,
      "loss": 2.4883,
      "step": 2850
    },
    {
      "epoch": 0.5459057071960298,
      "grad_norm": 18.59235382080078,
      "learning_rate": 9.143514083157693e-05,
      "loss": 2.2075,
      "step": 2860
    },
    {
      "epoch": 0.5478144684100019,
      "grad_norm": 9.08352279663086,
      "learning_rate": 9.105192565625598e-05,
      "loss": 1.9709,
      "step": 2870
    },
    {
      "epoch": 0.549723229623974,
      "grad_norm": 33.26404571533203,
      "learning_rate": 9.066871048093505e-05,
      "loss": 1.8903,
      "step": 2880
    },
    {
      "epoch": 0.5516319908379461,
      "grad_norm": 14.194899559020996,
      "learning_rate": 9.02854953056141e-05,
      "loss": 1.8925,
      "step": 2890
    },
    {
      "epoch": 0.5535407520519183,
      "grad_norm": 54.48274612426758,
      "learning_rate": 8.990228013029316e-05,
      "loss": 1.9341,
      "step": 2900
    },
    {
      "epoch": 0.5554495132658904,
      "grad_norm": 30.12903594970703,
      "learning_rate": 8.951906495497222e-05,
      "loss": 1.3098,
      "step": 2910
    },
    {
      "epoch": 0.5573582744798625,
      "grad_norm": 22.477643966674805,
      "learning_rate": 8.913584977965128e-05,
      "loss": 1.9462,
      "step": 2920
    },
    {
      "epoch": 0.5592670356938347,
      "grad_norm": 21.032297134399414,
      "learning_rate": 8.875263460433033e-05,
      "loss": 1.9071,
      "step": 2930
    },
    {
      "epoch": 0.5611757969078068,
      "grad_norm": 31.664615631103516,
      "learning_rate": 8.83694194290094e-05,
      "loss": 1.9346,
      "step": 2940
    },
    {
      "epoch": 0.5630845581217789,
      "grad_norm": 18.202251434326172,
      "learning_rate": 8.798620425368845e-05,
      "loss": 1.9692,
      "step": 2950
    },
    {
      "epoch": 0.5649933193357511,
      "grad_norm": 22.074909210205078,
      "learning_rate": 8.76029890783675e-05,
      "loss": 1.8391,
      "step": 2960
    },
    {
      "epoch": 0.5669020805497232,
      "grad_norm": 27.171602249145508,
      "learning_rate": 8.721977390304657e-05,
      "loss": 1.9666,
      "step": 2970
    },
    {
      "epoch": 0.5688108417636953,
      "grad_norm": 24.675691604614258,
      "learning_rate": 8.683655872772562e-05,
      "loss": 1.5656,
      "step": 2980
    },
    {
      "epoch": 0.5707196029776674,
      "grad_norm": 19.159648895263672,
      "learning_rate": 8.645334355240467e-05,
      "loss": 1.9671,
      "step": 2990
    },
    {
      "epoch": 0.5726283641916396,
      "grad_norm": 26.330961227416992,
      "learning_rate": 8.607012837708374e-05,
      "loss": 1.5791,
      "step": 3000
    },
    {
      "epoch": 0.5745371254056117,
      "grad_norm": 20.21739387512207,
      "learning_rate": 8.568691320176279e-05,
      "loss": 1.2994,
      "step": 3010
    },
    {
      "epoch": 0.5764458866195838,
      "grad_norm": 12.494421005249023,
      "learning_rate": 8.530369802644184e-05,
      "loss": 1.1864,
      "step": 3020
    },
    {
      "epoch": 0.5783546478335561,
      "grad_norm": 36.78330612182617,
      "learning_rate": 8.492048285112091e-05,
      "loss": 2.0843,
      "step": 3030
    },
    {
      "epoch": 0.5802634090475282,
      "grad_norm": 26.73807144165039,
      "learning_rate": 8.453726767579996e-05,
      "loss": 2.1592,
      "step": 3040
    },
    {
      "epoch": 0.5821721702615003,
      "grad_norm": 35.54622268676758,
      "learning_rate": 8.415405250047902e-05,
      "loss": 1.6126,
      "step": 3050
    },
    {
      "epoch": 0.5840809314754725,
      "grad_norm": 20.141815185546875,
      "learning_rate": 8.377083732515808e-05,
      "loss": 1.5326,
      "step": 3060
    },
    {
      "epoch": 0.5859896926894446,
      "grad_norm": 32.8698616027832,
      "learning_rate": 8.338762214983715e-05,
      "loss": 2.0208,
      "step": 3070
    },
    {
      "epoch": 0.5878984539034167,
      "grad_norm": 14.916192054748535,
      "learning_rate": 8.300440697451619e-05,
      "loss": 1.523,
      "step": 3080
    },
    {
      "epoch": 0.5898072151173888,
      "grad_norm": 35.51665115356445,
      "learning_rate": 8.262119179919525e-05,
      "loss": 2.0134,
      "step": 3090
    },
    {
      "epoch": 0.591715976331361,
      "grad_norm": 40.07572555541992,
      "learning_rate": 8.223797662387432e-05,
      "loss": 2.1829,
      "step": 3100
    },
    {
      "epoch": 0.5936247375453331,
      "grad_norm": 21.728805541992188,
      "learning_rate": 8.185476144855336e-05,
      "loss": 1.7571,
      "step": 3110
    },
    {
      "epoch": 0.5955334987593052,
      "grad_norm": 21.351760864257812,
      "learning_rate": 8.147154627323243e-05,
      "loss": 1.689,
      "step": 3120
    },
    {
      "epoch": 0.5974422599732774,
      "grad_norm": 16.914487838745117,
      "learning_rate": 8.108833109791149e-05,
      "loss": 1.529,
      "step": 3130
    },
    {
      "epoch": 0.5993510211872495,
      "grad_norm": 17.198467254638672,
      "learning_rate": 8.070511592259053e-05,
      "loss": 1.8057,
      "step": 3140
    },
    {
      "epoch": 0.6012597824012216,
      "grad_norm": 18.374282836914062,
      "learning_rate": 8.03219007472696e-05,
      "loss": 1.8703,
      "step": 3150
    },
    {
      "epoch": 0.6031685436151938,
      "grad_norm": 23.454565048217773,
      "learning_rate": 7.993868557194866e-05,
      "loss": 2.0886,
      "step": 3160
    },
    {
      "epoch": 0.6050773048291659,
      "grad_norm": 16.30510902404785,
      "learning_rate": 7.95554703966277e-05,
      "loss": 1.7987,
      "step": 3170
    },
    {
      "epoch": 0.606986066043138,
      "grad_norm": 15.57827091217041,
      "learning_rate": 7.917225522130677e-05,
      "loss": 2.0785,
      "step": 3180
    },
    {
      "epoch": 0.6088948272571101,
      "grad_norm": 28.15146255493164,
      "learning_rate": 7.878904004598584e-05,
      "loss": 1.5271,
      "step": 3190
    },
    {
      "epoch": 0.6108035884710823,
      "grad_norm": 18.053075790405273,
      "learning_rate": 7.840582487066487e-05,
      "loss": 1.9877,
      "step": 3200
    },
    {
      "epoch": 0.6127123496850544,
      "grad_norm": 11.549476623535156,
      "learning_rate": 7.802260969534394e-05,
      "loss": 1.9501,
      "step": 3210
    },
    {
      "epoch": 0.6146211108990265,
      "grad_norm": 11.465880393981934,
      "learning_rate": 7.763939452002301e-05,
      "loss": 1.7474,
      "step": 3220
    },
    {
      "epoch": 0.6165298721129987,
      "grad_norm": 27.309467315673828,
      "learning_rate": 7.725617934470205e-05,
      "loss": 2.1357,
      "step": 3230
    },
    {
      "epoch": 0.6184386333269708,
      "grad_norm": 27.606483459472656,
      "learning_rate": 7.687296416938111e-05,
      "loss": 1.8052,
      "step": 3240
    },
    {
      "epoch": 0.6203473945409429,
      "grad_norm": 17.601869583129883,
      "learning_rate": 7.648974899406018e-05,
      "loss": 1.5462,
      "step": 3250
    },
    {
      "epoch": 0.6222561557549151,
      "grad_norm": 35.570152282714844,
      "learning_rate": 7.610653381873922e-05,
      "loss": 1.8629,
      "step": 3260
    },
    {
      "epoch": 0.6241649169688872,
      "grad_norm": 37.2000846862793,
      "learning_rate": 7.572331864341828e-05,
      "loss": 1.7328,
      "step": 3270
    },
    {
      "epoch": 0.6260736781828593,
      "grad_norm": 50.82594680786133,
      "learning_rate": 7.534010346809735e-05,
      "loss": 1.8583,
      "step": 3280
    },
    {
      "epoch": 0.6279824393968314,
      "grad_norm": 16.91748809814453,
      "learning_rate": 7.495688829277639e-05,
      "loss": 1.5232,
      "step": 3290
    },
    {
      "epoch": 0.6298912006108036,
      "grad_norm": 14.678521156311035,
      "learning_rate": 7.457367311745546e-05,
      "loss": 1.815,
      "step": 3300
    },
    {
      "epoch": 0.6317999618247757,
      "grad_norm": 21.358854293823242,
      "learning_rate": 7.419045794213452e-05,
      "loss": 1.8031,
      "step": 3310
    },
    {
      "epoch": 0.6337087230387478,
      "grad_norm": 16.504344940185547,
      "learning_rate": 7.380724276681356e-05,
      "loss": 1.7689,
      "step": 3320
    },
    {
      "epoch": 0.63561748425272,
      "grad_norm": 41.7763557434082,
      "learning_rate": 7.342402759149263e-05,
      "loss": 1.6032,
      "step": 3330
    },
    {
      "epoch": 0.6375262454666921,
      "grad_norm": 23.770217895507812,
      "learning_rate": 7.30408124161717e-05,
      "loss": 1.8988,
      "step": 3340
    },
    {
      "epoch": 0.6394350066806642,
      "grad_norm": 29.940109252929688,
      "learning_rate": 7.265759724085073e-05,
      "loss": 1.759,
      "step": 3350
    },
    {
      "epoch": 0.6413437678946364,
      "grad_norm": 15.3639497756958,
      "learning_rate": 7.22743820655298e-05,
      "loss": 1.6186,
      "step": 3360
    },
    {
      "epoch": 0.6432525291086085,
      "grad_norm": 19.242992401123047,
      "learning_rate": 7.189116689020887e-05,
      "loss": 1.4435,
      "step": 3370
    },
    {
      "epoch": 0.6451612903225806,
      "grad_norm": 30.031349182128906,
      "learning_rate": 7.15079517148879e-05,
      "loss": 1.8104,
      "step": 3380
    },
    {
      "epoch": 0.6470700515365527,
      "grad_norm": 10.205781936645508,
      "learning_rate": 7.112473653956697e-05,
      "loss": 1.1068,
      "step": 3390
    },
    {
      "epoch": 0.6489788127505249,
      "grad_norm": 47.92844009399414,
      "learning_rate": 7.074152136424604e-05,
      "loss": 1.9695,
      "step": 3400
    },
    {
      "epoch": 0.650887573964497,
      "grad_norm": 10.73436164855957,
      "learning_rate": 7.035830618892508e-05,
      "loss": 1.727,
      "step": 3410
    },
    {
      "epoch": 0.6527963351784691,
      "grad_norm": 23.90855598449707,
      "learning_rate": 6.997509101360414e-05,
      "loss": 1.9067,
      "step": 3420
    },
    {
      "epoch": 0.6547050963924413,
      "grad_norm": 9.04831314086914,
      "learning_rate": 6.959187583828321e-05,
      "loss": 1.7775,
      "step": 3430
    },
    {
      "epoch": 0.6566138576064134,
      "grad_norm": 16.581989288330078,
      "learning_rate": 6.920866066296225e-05,
      "loss": 1.7448,
      "step": 3440
    },
    {
      "epoch": 0.6585226188203855,
      "grad_norm": 31.899887084960938,
      "learning_rate": 6.882544548764132e-05,
      "loss": 1.8197,
      "step": 3450
    },
    {
      "epoch": 0.6604313800343578,
      "grad_norm": 27.62629508972168,
      "learning_rate": 6.844223031232037e-05,
      "loss": 2.0404,
      "step": 3460
    },
    {
      "epoch": 0.6623401412483299,
      "grad_norm": 51.91705322265625,
      "learning_rate": 6.805901513699942e-05,
      "loss": 1.6988,
      "step": 3470
    },
    {
      "epoch": 0.664248902462302,
      "grad_norm": 17.72865867614746,
      "learning_rate": 6.767579996167849e-05,
      "loss": 1.8239,
      "step": 3480
    },
    {
      "epoch": 0.666157663676274,
      "grad_norm": 37.322845458984375,
      "learning_rate": 6.729258478635754e-05,
      "loss": 1.8251,
      "step": 3490
    },
    {
      "epoch": 0.6680664248902463,
      "grad_norm": 31.101757049560547,
      "learning_rate": 6.69093696110366e-05,
      "loss": 1.6018,
      "step": 3500
    },
    {
      "epoch": 0.6699751861042184,
      "grad_norm": 21.051782608032227,
      "learning_rate": 6.652615443571566e-05,
      "loss": 1.9347,
      "step": 3510
    },
    {
      "epoch": 0.6718839473181905,
      "grad_norm": 19.47994041442871,
      "learning_rate": 6.614293926039471e-05,
      "loss": 1.4622,
      "step": 3520
    },
    {
      "epoch": 0.6737927085321627,
      "grad_norm": 34.589141845703125,
      "learning_rate": 6.575972408507377e-05,
      "loss": 1.9652,
      "step": 3530
    },
    {
      "epoch": 0.6757014697461348,
      "grad_norm": 30.3873348236084,
      "learning_rate": 6.537650890975283e-05,
      "loss": 2.0395,
      "step": 3540
    },
    {
      "epoch": 0.6776102309601069,
      "grad_norm": 16.49127960205078,
      "learning_rate": 6.499329373443188e-05,
      "loss": 2.0605,
      "step": 3550
    },
    {
      "epoch": 0.6795189921740791,
      "grad_norm": 29.94991111755371,
      "learning_rate": 6.461007855911094e-05,
      "loss": 1.3901,
      "step": 3560
    },
    {
      "epoch": 0.6814277533880512,
      "grad_norm": 14.712517738342285,
      "learning_rate": 6.422686338379e-05,
      "loss": 1.6393,
      "step": 3570
    },
    {
      "epoch": 0.6833365146020233,
      "grad_norm": 21.634084701538086,
      "learning_rate": 6.384364820846906e-05,
      "loss": 1.7075,
      "step": 3580
    },
    {
      "epoch": 0.6852452758159954,
      "grad_norm": 26.046369552612305,
      "learning_rate": 6.346043303314811e-05,
      "loss": 1.9341,
      "step": 3590
    },
    {
      "epoch": 0.6871540370299676,
      "grad_norm": 16.716508865356445,
      "learning_rate": 6.307721785782718e-05,
      "loss": 1.6679,
      "step": 3600
    },
    {
      "epoch": 0.6890627982439397,
      "grad_norm": 7.311105251312256,
      "learning_rate": 6.269400268250623e-05,
      "loss": 1.8654,
      "step": 3610
    },
    {
      "epoch": 0.6909715594579118,
      "grad_norm": 26.782100677490234,
      "learning_rate": 6.231078750718528e-05,
      "loss": 2.3401,
      "step": 3620
    },
    {
      "epoch": 0.692880320671884,
      "grad_norm": 28.831893920898438,
      "learning_rate": 6.192757233186435e-05,
      "loss": 1.9572,
      "step": 3630
    },
    {
      "epoch": 0.6947890818858561,
      "grad_norm": 29.89865493774414,
      "learning_rate": 6.15443571565434e-05,
      "loss": 1.6007,
      "step": 3640
    },
    {
      "epoch": 0.6966978430998282,
      "grad_norm": 12.887292861938477,
      "learning_rate": 6.116114198122245e-05,
      "loss": 1.7196,
      "step": 3650
    },
    {
      "epoch": 0.6986066043138004,
      "grad_norm": 34.06033706665039,
      "learning_rate": 6.077792680590152e-05,
      "loss": 1.69,
      "step": 3660
    },
    {
      "epoch": 0.7005153655277725,
      "grad_norm": 10.757140159606934,
      "learning_rate": 6.039471163058057e-05,
      "loss": 2.0457,
      "step": 3670
    },
    {
      "epoch": 0.7024241267417446,
      "grad_norm": 31.963472366333008,
      "learning_rate": 6.001149645525963e-05,
      "loss": 1.4773,
      "step": 3680
    },
    {
      "epoch": 0.7043328879557167,
      "grad_norm": 16.338314056396484,
      "learning_rate": 5.962828127993869e-05,
      "loss": 1.555,
      "step": 3690
    },
    {
      "epoch": 0.7062416491696889,
      "grad_norm": 19.713029861450195,
      "learning_rate": 5.9245066104617744e-05,
      "loss": 1.8959,
      "step": 3700
    },
    {
      "epoch": 0.708150410383661,
      "grad_norm": 29.229042053222656,
      "learning_rate": 5.8861850929296804e-05,
      "loss": 1.5289,
      "step": 3710
    },
    {
      "epoch": 0.7100591715976331,
      "grad_norm": 14.286532402038574,
      "learning_rate": 5.847863575397586e-05,
      "loss": 1.5311,
      "step": 3720
    },
    {
      "epoch": 0.7119679328116053,
      "grad_norm": 13.40322494506836,
      "learning_rate": 5.8095420578654916e-05,
      "loss": 1.5326,
      "step": 3730
    },
    {
      "epoch": 0.7138766940255774,
      "grad_norm": 31.049345016479492,
      "learning_rate": 5.7712205403333975e-05,
      "loss": 1.5359,
      "step": 3740
    },
    {
      "epoch": 0.7157854552395495,
      "grad_norm": 22.769084930419922,
      "learning_rate": 5.7328990228013035e-05,
      "loss": 1.2443,
      "step": 3750
    },
    {
      "epoch": 0.7176942164535217,
      "grad_norm": 42.22285842895508,
      "learning_rate": 5.694577505269209e-05,
      "loss": 1.7107,
      "step": 3760
    },
    {
      "epoch": 0.7196029776674938,
      "grad_norm": 40.0417366027832,
      "learning_rate": 5.656255987737115e-05,
      "loss": 1.9774,
      "step": 3770
    },
    {
      "epoch": 0.7215117388814659,
      "grad_norm": 17.21926498413086,
      "learning_rate": 5.617934470205021e-05,
      "loss": 1.1857,
      "step": 3780
    },
    {
      "epoch": 0.723420500095438,
      "grad_norm": 15.494878768920898,
      "learning_rate": 5.579612952672926e-05,
      "loss": 1.7277,
      "step": 3790
    },
    {
      "epoch": 0.7253292613094102,
      "grad_norm": 17.809940338134766,
      "learning_rate": 5.541291435140832e-05,
      "loss": 1.9175,
      "step": 3800
    },
    {
      "epoch": 0.7272380225233823,
      "grad_norm": 15.469783782958984,
      "learning_rate": 5.502969917608738e-05,
      "loss": 1.8964,
      "step": 3810
    },
    {
      "epoch": 0.7291467837373544,
      "grad_norm": 24.922964096069336,
      "learning_rate": 5.464648400076643e-05,
      "loss": 1.5449,
      "step": 3820
    },
    {
      "epoch": 0.7310555449513266,
      "grad_norm": 17.925142288208008,
      "learning_rate": 5.426326882544549e-05,
      "loss": 1.8766,
      "step": 3830
    },
    {
      "epoch": 0.7329643061652987,
      "grad_norm": 47.91225051879883,
      "learning_rate": 5.388005365012455e-05,
      "loss": 1.707,
      "step": 3840
    },
    {
      "epoch": 0.7348730673792708,
      "grad_norm": 38.329444885253906,
      "learning_rate": 5.3496838474803603e-05,
      "loss": 1.8455,
      "step": 3850
    },
    {
      "epoch": 0.736781828593243,
      "grad_norm": 33.013267517089844,
      "learning_rate": 5.311362329948266e-05,
      "loss": 1.8626,
      "step": 3860
    },
    {
      "epoch": 0.7386905898072151,
      "grad_norm": 28.71660804748535,
      "learning_rate": 5.273040812416172e-05,
      "loss": 1.4481,
      "step": 3870
    },
    {
      "epoch": 0.7405993510211872,
      "grad_norm": 20.434518814086914,
      "learning_rate": 5.2347192948840775e-05,
      "loss": 1.8928,
      "step": 3880
    },
    {
      "epoch": 0.7425081122351593,
      "grad_norm": 51.027950286865234,
      "learning_rate": 5.1963977773519835e-05,
      "loss": 1.7365,
      "step": 3890
    },
    {
      "epoch": 0.7444168734491315,
      "grad_norm": 21.623016357421875,
      "learning_rate": 5.1580762598198894e-05,
      "loss": 1.6417,
      "step": 3900
    },
    {
      "epoch": 0.7463256346631036,
      "grad_norm": 10.603314399719238,
      "learning_rate": 5.119754742287795e-05,
      "loss": 2.0012,
      "step": 3910
    },
    {
      "epoch": 0.7482343958770757,
      "grad_norm": 14.28961181640625,
      "learning_rate": 5.081433224755701e-05,
      "loss": 1.5337,
      "step": 3920
    },
    {
      "epoch": 0.750143157091048,
      "grad_norm": 21.314504623413086,
      "learning_rate": 5.0431117072236066e-05,
      "loss": 1.7847,
      "step": 3930
    },
    {
      "epoch": 0.75205191830502,
      "grad_norm": 20.983224868774414,
      "learning_rate": 5.004790189691512e-05,
      "loss": 1.7251,
      "step": 3940
    },
    {
      "epoch": 0.7539606795189921,
      "grad_norm": 22.469165802001953,
      "learning_rate": 4.966468672159418e-05,
      "loss": 1.3976,
      "step": 3950
    },
    {
      "epoch": 0.7558694407329644,
      "grad_norm": 26.287643432617188,
      "learning_rate": 4.928147154627323e-05,
      "loss": 2.1034,
      "step": 3960
    },
    {
      "epoch": 0.7577782019469365,
      "grad_norm": 24.58184814453125,
      "learning_rate": 4.88982563709523e-05,
      "loss": 1.9479,
      "step": 3970
    },
    {
      "epoch": 0.7596869631609086,
      "grad_norm": 35.02473449707031,
      "learning_rate": 4.851504119563135e-05,
      "loss": 1.6553,
      "step": 3980
    },
    {
      "epoch": 0.7615957243748807,
      "grad_norm": 23.458477020263672,
      "learning_rate": 4.81318260203104e-05,
      "loss": 1.5359,
      "step": 3990
    },
    {
      "epoch": 0.7635044855888529,
      "grad_norm": 28.383319854736328,
      "learning_rate": 4.774861084498947e-05,
      "loss": 1.9799,
      "step": 4000
    },
    {
      "epoch": 0.765413246802825,
      "grad_norm": 30.099390029907227,
      "learning_rate": 4.736539566966852e-05,
      "loss": 2.2459,
      "step": 4010
    },
    {
      "epoch": 0.7673220080167971,
      "grad_norm": 9.679327964782715,
      "learning_rate": 4.6982180494347575e-05,
      "loss": 1.4334,
      "step": 4020
    },
    {
      "epoch": 0.7692307692307693,
      "grad_norm": 13.025967597961426,
      "learning_rate": 4.659896531902664e-05,
      "loss": 1.3456,
      "step": 4030
    },
    {
      "epoch": 0.7711395304447414,
      "grad_norm": 19.070314407348633,
      "learning_rate": 4.6215750143705694e-05,
      "loss": 1.6304,
      "step": 4040
    },
    {
      "epoch": 0.7730482916587135,
      "grad_norm": 9.487524032592773,
      "learning_rate": 4.583253496838475e-05,
      "loss": 1.6364,
      "step": 4050
    },
    {
      "epoch": 0.7749570528726857,
      "grad_norm": 16.368045806884766,
      "learning_rate": 4.544931979306381e-05,
      "loss": 2.3529,
      "step": 4060
    },
    {
      "epoch": 0.7768658140866578,
      "grad_norm": 21.98301887512207,
      "learning_rate": 4.5066104617742866e-05,
      "loss": 1.6175,
      "step": 4070
    },
    {
      "epoch": 0.7787745753006299,
      "grad_norm": 14.710800170898438,
      "learning_rate": 4.468288944242192e-05,
      "loss": 1.606,
      "step": 4080
    },
    {
      "epoch": 0.780683336514602,
      "grad_norm": 41.22636032104492,
      "learning_rate": 4.429967426710098e-05,
      "loss": 1.7199,
      "step": 4090
    },
    {
      "epoch": 0.7825920977285742,
      "grad_norm": 24.6669921875,
      "learning_rate": 4.391645909178004e-05,
      "loss": 1.8511,
      "step": 4100
    },
    {
      "epoch": 0.7845008589425463,
      "grad_norm": 16.388229370117188,
      "learning_rate": 4.353324391645909e-05,
      "loss": 1.423,
      "step": 4110
    },
    {
      "epoch": 0.7864096201565184,
      "grad_norm": 23.062240600585938,
      "learning_rate": 4.315002874113815e-05,
      "loss": 1.2972,
      "step": 4120
    },
    {
      "epoch": 0.7883183813704906,
      "grad_norm": 26.820270538330078,
      "learning_rate": 4.276681356581721e-05,
      "loss": 1.7813,
      "step": 4130
    },
    {
      "epoch": 0.7902271425844627,
      "grad_norm": 25.08930778503418,
      "learning_rate": 4.238359839049626e-05,
      "loss": 1.7548,
      "step": 4140
    },
    {
      "epoch": 0.7921359037984348,
      "grad_norm": 15.45607852935791,
      "learning_rate": 4.200038321517532e-05,
      "loss": 1.4903,
      "step": 4150
    },
    {
      "epoch": 0.794044665012407,
      "grad_norm": 32.055728912353516,
      "learning_rate": 4.161716803985438e-05,
      "loss": 1.5482,
      "step": 4160
    },
    {
      "epoch": 0.7959534262263791,
      "grad_norm": 25.903783798217773,
      "learning_rate": 4.1233952864533435e-05,
      "loss": 1.4735,
      "step": 4170
    },
    {
      "epoch": 0.7978621874403512,
      "grad_norm": 19.56939125061035,
      "learning_rate": 4.0850737689212494e-05,
      "loss": 1.9428,
      "step": 4180
    },
    {
      "epoch": 0.7997709486543233,
      "grad_norm": 49.91209411621094,
      "learning_rate": 4.0467522513891554e-05,
      "loss": 1.6966,
      "step": 4190
    },
    {
      "epoch": 0.8016797098682955,
      "grad_norm": 20.4332218170166,
      "learning_rate": 4.0084307338570607e-05,
      "loss": 1.7648,
      "step": 4200
    },
    {
      "epoch": 0.8035884710822676,
      "grad_norm": 23.781110763549805,
      "learning_rate": 3.9701092163249666e-05,
      "loss": 2.0677,
      "step": 4210
    },
    {
      "epoch": 0.8054972322962397,
      "grad_norm": 17.524259567260742,
      "learning_rate": 3.9317876987928726e-05,
      "loss": 1.3883,
      "step": 4220
    },
    {
      "epoch": 0.8074059935102119,
      "grad_norm": 14.090812683105469,
      "learning_rate": 3.893466181260778e-05,
      "loss": 1.8658,
      "step": 4230
    },
    {
      "epoch": 0.809314754724184,
      "grad_norm": 21.501480102539062,
      "learning_rate": 3.855144663728684e-05,
      "loss": 1.4477,
      "step": 4240
    },
    {
      "epoch": 0.8112235159381561,
      "grad_norm": 29.843660354614258,
      "learning_rate": 3.81682314619659e-05,
      "loss": 1.8459,
      "step": 4250
    },
    {
      "epoch": 0.8131322771521282,
      "grad_norm": 15.035752296447754,
      "learning_rate": 3.778501628664495e-05,
      "loss": 1.6841,
      "step": 4260
    },
    {
      "epoch": 0.8150410383661004,
      "grad_norm": 18.92511558532715,
      "learning_rate": 3.740180111132401e-05,
      "loss": 1.4944,
      "step": 4270
    },
    {
      "epoch": 0.8169497995800725,
      "grad_norm": 14.90921401977539,
      "learning_rate": 3.701858593600307e-05,
      "loss": 1.6504,
      "step": 4280
    },
    {
      "epoch": 0.8188585607940446,
      "grad_norm": 20.508960723876953,
      "learning_rate": 3.663537076068212e-05,
      "loss": 1.6745,
      "step": 4290
    },
    {
      "epoch": 0.8207673220080168,
      "grad_norm": 18.439741134643555,
      "learning_rate": 3.625215558536118e-05,
      "loss": 1.7078,
      "step": 4300
    },
    {
      "epoch": 0.8226760832219889,
      "grad_norm": 22.35541343688965,
      "learning_rate": 3.586894041004024e-05,
      "loss": 1.6293,
      "step": 4310
    },
    {
      "epoch": 0.824584844435961,
      "grad_norm": 52.63574981689453,
      "learning_rate": 3.5485725234719294e-05,
      "loss": 1.8905,
      "step": 4320
    },
    {
      "epoch": 0.8264936056499332,
      "grad_norm": 14.059685707092285,
      "learning_rate": 3.5102510059398354e-05,
      "loss": 1.6281,
      "step": 4330
    },
    {
      "epoch": 0.8284023668639053,
      "grad_norm": 35.02359390258789,
      "learning_rate": 3.471929488407741e-05,
      "loss": 1.2666,
      "step": 4340
    },
    {
      "epoch": 0.8303111280778774,
      "grad_norm": 37.65913009643555,
      "learning_rate": 3.4336079708756466e-05,
      "loss": 1.6546,
      "step": 4350
    },
    {
      "epoch": 0.8322198892918495,
      "grad_norm": 23.28074073791504,
      "learning_rate": 3.3952864533435525e-05,
      "loss": 1.819,
      "step": 4360
    },
    {
      "epoch": 0.8341286505058217,
      "grad_norm": 27.251235961914062,
      "learning_rate": 3.3569649358114585e-05,
      "loss": 1.5633,
      "step": 4370
    },
    {
      "epoch": 0.8360374117197938,
      "grad_norm": 36.649681091308594,
      "learning_rate": 3.318643418279364e-05,
      "loss": 2.1838,
      "step": 4380
    },
    {
      "epoch": 0.8379461729337659,
      "grad_norm": 11.020853996276855,
      "learning_rate": 3.28032190074727e-05,
      "loss": 1.6551,
      "step": 4390
    },
    {
      "epoch": 0.8398549341477382,
      "grad_norm": 41.14567947387695,
      "learning_rate": 3.242000383215176e-05,
      "loss": 1.9626,
      "step": 4400
    },
    {
      "epoch": 0.8417636953617103,
      "grad_norm": 8.094059944152832,
      "learning_rate": 3.203678865683081e-05,
      "loss": 1.603,
      "step": 4410
    },
    {
      "epoch": 0.8436724565756824,
      "grad_norm": 51.74886703491211,
      "learning_rate": 3.165357348150987e-05,
      "loss": 1.8018,
      "step": 4420
    },
    {
      "epoch": 0.8455812177896546,
      "grad_norm": 16.177947998046875,
      "learning_rate": 3.127035830618892e-05,
      "loss": 1.6121,
      "step": 4430
    },
    {
      "epoch": 0.8474899790036267,
      "grad_norm": 40.12288284301758,
      "learning_rate": 3.088714313086798e-05,
      "loss": 1.6866,
      "step": 4440
    },
    {
      "epoch": 0.8493987402175988,
      "grad_norm": 15.713089942932129,
      "learning_rate": 3.050392795554704e-05,
      "loss": 1.3357,
      "step": 4450
    },
    {
      "epoch": 0.8513075014315709,
      "grad_norm": 26.398117065429688,
      "learning_rate": 3.0120712780226097e-05,
      "loss": 1.5918,
      "step": 4460
    },
    {
      "epoch": 0.8532162626455431,
      "grad_norm": 35.46320343017578,
      "learning_rate": 2.9737497604905157e-05,
      "loss": 1.792,
      "step": 4470
    },
    {
      "epoch": 0.8551250238595152,
      "grad_norm": 18.048189163208008,
      "learning_rate": 2.9354282429584213e-05,
      "loss": 2.0542,
      "step": 4480
    },
    {
      "epoch": 0.8570337850734873,
      "grad_norm": 28.338632583618164,
      "learning_rate": 2.897106725426327e-05,
      "loss": 1.9324,
      "step": 4490
    },
    {
      "epoch": 0.8589425462874595,
      "grad_norm": 19.812179565429688,
      "learning_rate": 2.858785207894233e-05,
      "loss": 1.9242,
      "step": 4500
    },
    {
      "epoch": 0.8608513075014316,
      "grad_norm": 21.18712043762207,
      "learning_rate": 2.8204636903621385e-05,
      "loss": 1.3934,
      "step": 4510
    },
    {
      "epoch": 0.8627600687154037,
      "grad_norm": 35.7081413269043,
      "learning_rate": 2.782142172830044e-05,
      "loss": 2.051,
      "step": 4520
    },
    {
      "epoch": 0.8646688299293759,
      "grad_norm": 20.43448257446289,
      "learning_rate": 2.74382065529795e-05,
      "loss": 1.8845,
      "step": 4530
    },
    {
      "epoch": 0.866577591143348,
      "grad_norm": 13.78852367401123,
      "learning_rate": 2.7054991377658557e-05,
      "loss": 1.4649,
      "step": 4540
    },
    {
      "epoch": 0.8684863523573201,
      "grad_norm": 34.33990478515625,
      "learning_rate": 2.6671776202337613e-05,
      "loss": 1.4712,
      "step": 4550
    },
    {
      "epoch": 0.8703951135712922,
      "grad_norm": 11.537717819213867,
      "learning_rate": 2.6288561027016673e-05,
      "loss": 1.36,
      "step": 4560
    },
    {
      "epoch": 0.8723038747852644,
      "grad_norm": 47.931419372558594,
      "learning_rate": 2.590534585169573e-05,
      "loss": 1.9233,
      "step": 4570
    },
    {
      "epoch": 0.8742126359992365,
      "grad_norm": 12.171246528625488,
      "learning_rate": 2.5522130676374785e-05,
      "loss": 1.3978,
      "step": 4580
    },
    {
      "epoch": 0.8761213972132086,
      "grad_norm": 24.755586624145508,
      "learning_rate": 2.5138915501053844e-05,
      "loss": 1.8104,
      "step": 4590
    },
    {
      "epoch": 0.8780301584271808,
      "grad_norm": 37.35297775268555,
      "learning_rate": 2.47557003257329e-05,
      "loss": 1.3096,
      "step": 4600
    },
    {
      "epoch": 0.8799389196411529,
      "grad_norm": 24.27645492553711,
      "learning_rate": 2.4372485150411957e-05,
      "loss": 1.3244,
      "step": 4610
    },
    {
      "epoch": 0.881847680855125,
      "grad_norm": 17.2185001373291,
      "learning_rate": 2.3989269975091013e-05,
      "loss": 2.0724,
      "step": 4620
    },
    {
      "epoch": 0.8837564420690972,
      "grad_norm": 38.0059814453125,
      "learning_rate": 2.3606054799770072e-05,
      "loss": 1.9982,
      "step": 4630
    },
    {
      "epoch": 0.8856652032830693,
      "grad_norm": 22.154373168945312,
      "learning_rate": 2.322283962444913e-05,
      "loss": 1.2071,
      "step": 4640
    },
    {
      "epoch": 0.8875739644970414,
      "grad_norm": 34.34994125366211,
      "learning_rate": 2.2839624449128185e-05,
      "loss": 1.8002,
      "step": 4650
    },
    {
      "epoch": 0.8894827257110135,
      "grad_norm": 22.56011962890625,
      "learning_rate": 2.2456409273807244e-05,
      "loss": 1.4418,
      "step": 4660
    },
    {
      "epoch": 0.8913914869249857,
      "grad_norm": 17.782487869262695,
      "learning_rate": 2.2073194098486304e-05,
      "loss": 1.7022,
      "step": 4670
    },
    {
      "epoch": 0.8933002481389578,
      "grad_norm": 36.30180740356445,
      "learning_rate": 2.1689978923165357e-05,
      "loss": 1.5911,
      "step": 4680
    },
    {
      "epoch": 0.8952090093529299,
      "grad_norm": 31.00165557861328,
      "learning_rate": 2.1306763747844416e-05,
      "loss": 2.1195,
      "step": 4690
    },
    {
      "epoch": 0.8971177705669021,
      "grad_norm": 17.272109985351562,
      "learning_rate": 2.0923548572523476e-05,
      "loss": 1.3255,
      "step": 4700
    },
    {
      "epoch": 0.8990265317808742,
      "grad_norm": 44.46832275390625,
      "learning_rate": 2.054033339720253e-05,
      "loss": 1.5377,
      "step": 4710
    },
    {
      "epoch": 0.9009352929948463,
      "grad_norm": 28.86397361755371,
      "learning_rate": 2.0157118221881588e-05,
      "loss": 1.3004,
      "step": 4720
    },
    {
      "epoch": 0.9028440542088185,
      "grad_norm": 14.549747467041016,
      "learning_rate": 1.9773903046560644e-05,
      "loss": 1.5782,
      "step": 4730
    },
    {
      "epoch": 0.9047528154227906,
      "grad_norm": 21.480581283569336,
      "learning_rate": 1.93906878712397e-05,
      "loss": 1.6025,
      "step": 4740
    },
    {
      "epoch": 0.9066615766367627,
      "grad_norm": 50.75455856323242,
      "learning_rate": 1.900747269591876e-05,
      "loss": 2.1345,
      "step": 4750
    },
    {
      "epoch": 0.9085703378507348,
      "grad_norm": 20.03791046142578,
      "learning_rate": 1.8624257520597816e-05,
      "loss": 1.4958,
      "step": 4760
    },
    {
      "epoch": 0.910479099064707,
      "grad_norm": 18.100793838500977,
      "learning_rate": 1.8241042345276872e-05,
      "loss": 1.7296,
      "step": 4770
    },
    {
      "epoch": 0.9123878602786791,
      "grad_norm": 15.802727699279785,
      "learning_rate": 1.7857827169955932e-05,
      "loss": 1.7842,
      "step": 4780
    },
    {
      "epoch": 0.9142966214926512,
      "grad_norm": 18.582975387573242,
      "learning_rate": 1.7474611994634988e-05,
      "loss": 1.9261,
      "step": 4790
    },
    {
      "epoch": 0.9162053827066234,
      "grad_norm": 17.668790817260742,
      "learning_rate": 1.7091396819314044e-05,
      "loss": 1.6664,
      "step": 4800
    },
    {
      "epoch": 0.9181141439205955,
      "grad_norm": 21.63711929321289,
      "learning_rate": 1.6708181643993104e-05,
      "loss": 1.6638,
      "step": 4810
    },
    {
      "epoch": 0.9200229051345676,
      "grad_norm": 27.469650268554688,
      "learning_rate": 1.632496646867216e-05,
      "loss": 1.8795,
      "step": 4820
    },
    {
      "epoch": 0.9219316663485398,
      "grad_norm": 41.387020111083984,
      "learning_rate": 1.5941751293351216e-05,
      "loss": 1.7932,
      "step": 4830
    },
    {
      "epoch": 0.923840427562512,
      "grad_norm": 22.385778427124023,
      "learning_rate": 1.5558536118030276e-05,
      "loss": 1.7437,
      "step": 4840
    },
    {
      "epoch": 0.925749188776484,
      "grad_norm": 7.25356388092041,
      "learning_rate": 1.5175320942709332e-05,
      "loss": 1.2889,
      "step": 4850
    },
    {
      "epoch": 0.9276579499904561,
      "grad_norm": 19.33733367919922,
      "learning_rate": 1.479210576738839e-05,
      "loss": 1.8943,
      "step": 4860
    },
    {
      "epoch": 0.9295667112044284,
      "grad_norm": 33.007896423339844,
      "learning_rate": 1.4408890592067448e-05,
      "loss": 1.9588,
      "step": 4870
    },
    {
      "epoch": 0.9314754724184005,
      "grad_norm": 21.999597549438477,
      "learning_rate": 1.4025675416746504e-05,
      "loss": 1.5983,
      "step": 4880
    },
    {
      "epoch": 0.9333842336323726,
      "grad_norm": 29.36699104309082,
      "learning_rate": 1.3642460241425562e-05,
      "loss": 2.1166,
      "step": 4890
    },
    {
      "epoch": 0.9352929948463448,
      "grad_norm": 27.237201690673828,
      "learning_rate": 1.325924506610462e-05,
      "loss": 1.4836,
      "step": 4900
    },
    {
      "epoch": 0.9372017560603169,
      "grad_norm": 33.61396026611328,
      "learning_rate": 1.2876029890783676e-05,
      "loss": 1.5901,
      "step": 4910
    },
    {
      "epoch": 0.939110517274289,
      "grad_norm": 16.28635597229004,
      "learning_rate": 1.2492814715462733e-05,
      "loss": 1.6889,
      "step": 4920
    },
    {
      "epoch": 0.9410192784882612,
      "grad_norm": 38.2059440612793,
      "learning_rate": 1.210959954014179e-05,
      "loss": 1.6267,
      "step": 4930
    },
    {
      "epoch": 0.9429280397022333,
      "grad_norm": 22.381675720214844,
      "learning_rate": 1.1726384364820847e-05,
      "loss": 1.4306,
      "step": 4940
    },
    {
      "epoch": 0.9448368009162054,
      "grad_norm": 22.73915672302246,
      "learning_rate": 1.1343169189499905e-05,
      "loss": 1.7649,
      "step": 4950
    },
    {
      "epoch": 0.9467455621301775,
      "grad_norm": 22.430889129638672,
      "learning_rate": 1.0959954014178961e-05,
      "loss": 2.0398,
      "step": 4960
    },
    {
      "epoch": 0.9486543233441497,
      "grad_norm": 11.402872085571289,
      "learning_rate": 1.057673883885802e-05,
      "loss": 1.653,
      "step": 4970
    },
    {
      "epoch": 0.9505630845581218,
      "grad_norm": 20.08702278137207,
      "learning_rate": 1.0193523663537076e-05,
      "loss": 1.6014,
      "step": 4980
    },
    {
      "epoch": 0.9524718457720939,
      "grad_norm": 22.022409439086914,
      "learning_rate": 9.810308488216133e-06,
      "loss": 1.6572,
      "step": 4990
    },
    {
      "epoch": 0.9543806069860661,
      "grad_norm": 7.994647026062012,
      "learning_rate": 9.427093312895191e-06,
      "loss": 1.7731,
      "step": 5000
    },
    {
      "epoch": 0.9562893682000382,
      "grad_norm": 13.750146865844727,
      "learning_rate": 9.043878137574247e-06,
      "loss": 1.4415,
      "step": 5010
    },
    {
      "epoch": 0.9581981294140103,
      "grad_norm": 21.466838836669922,
      "learning_rate": 8.660662962253307e-06,
      "loss": 1.8827,
      "step": 5020
    },
    {
      "epoch": 0.9601068906279825,
      "grad_norm": 32.88261032104492,
      "learning_rate": 8.277447786932363e-06,
      "loss": 1.7529,
      "step": 5030
    },
    {
      "epoch": 0.9620156518419546,
      "grad_norm": 34.989810943603516,
      "learning_rate": 7.89423261161142e-06,
      "loss": 1.9072,
      "step": 5040
    },
    {
      "epoch": 0.9639244130559267,
      "grad_norm": 28.58411407470703,
      "learning_rate": 7.511017436290478e-06,
      "loss": 1.8371,
      "step": 5050
    },
    {
      "epoch": 0.9658331742698988,
      "grad_norm": 29.11383628845215,
      "learning_rate": 7.127802260969535e-06,
      "loss": 2.0912,
      "step": 5060
    },
    {
      "epoch": 0.967741935483871,
      "grad_norm": 11.9562406539917,
      "learning_rate": 6.744587085648592e-06,
      "loss": 1.3533,
      "step": 5070
    },
    {
      "epoch": 0.9696506966978431,
      "grad_norm": 34.82512283325195,
      "learning_rate": 6.361371910327649e-06,
      "loss": 1.8337,
      "step": 5080
    },
    {
      "epoch": 0.9715594579118152,
      "grad_norm": 37.56337356567383,
      "learning_rate": 5.978156735006706e-06,
      "loss": 1.4207,
      "step": 5090
    },
    {
      "epoch": 0.9734682191257874,
      "grad_norm": 15.754816055297852,
      "learning_rate": 5.594941559685764e-06,
      "loss": 1.7864,
      "step": 5100
    },
    {
      "epoch": 0.9753769803397595,
      "grad_norm": 13.756799697875977,
      "learning_rate": 5.211726384364822e-06,
      "loss": 1.6256,
      "step": 5110
    },
    {
      "epoch": 0.9772857415537316,
      "grad_norm": 8.5587739944458,
      "learning_rate": 4.828511209043878e-06,
      "loss": 1.6096,
      "step": 5120
    },
    {
      "epoch": 0.9791945027677038,
      "grad_norm": 39.0029411315918,
      "learning_rate": 4.48361755125503e-06,
      "loss": 1.9875,
      "step": 5130
    },
    {
      "epoch": 0.9811032639816759,
      "grad_norm": 21.599454879760742,
      "learning_rate": 4.100402375934087e-06,
      "loss": 1.8398,
      "step": 5140
    },
    {
      "epoch": 0.983012025195648,
      "grad_norm": 23.79819679260254,
      "learning_rate": 3.7171872006131446e-06,
      "loss": 1.7693,
      "step": 5150
    },
    {
      "epoch": 0.9849207864096201,
      "grad_norm": 35.58321762084961,
      "learning_rate": 3.3339720252922016e-06,
      "loss": 1.22,
      "step": 5160
    },
    {
      "epoch": 0.9868295476235923,
      "grad_norm": 34.47877883911133,
      "learning_rate": 2.950756849971259e-06,
      "loss": 1.6018,
      "step": 5170
    },
    {
      "epoch": 0.9887383088375644,
      "grad_norm": 22.98208999633789,
      "learning_rate": 2.567541674650316e-06,
      "loss": 1.7485,
      "step": 5180
    },
    {
      "epoch": 0.9906470700515365,
      "grad_norm": 35.605594635009766,
      "learning_rate": 2.1843264993293735e-06,
      "loss": 1.4726,
      "step": 5190
    },
    {
      "epoch": 0.9925558312655087,
      "grad_norm": 31.135072708129883,
      "learning_rate": 1.801111324008431e-06,
      "loss": 1.9518,
      "step": 5200
    },
    {
      "epoch": 0.9944645924794808,
      "grad_norm": 32.90400314331055,
      "learning_rate": 1.4178961486874882e-06,
      "loss": 2.1206,
      "step": 5210
    },
    {
      "epoch": 0.9963733536934529,
      "grad_norm": 23.30251121520996,
      "learning_rate": 1.0346809733665454e-06,
      "loss": 1.7741,
      "step": 5220
    },
    {
      "epoch": 0.9982821149074251,
      "grad_norm": 16.826339721679688,
      "learning_rate": 6.514657980456027e-07,
      "loss": 1.5521,
      "step": 5230
    }
  ],
  "logging_steps": 10,
  "max_steps": 5239,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 8.363133592519649e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
