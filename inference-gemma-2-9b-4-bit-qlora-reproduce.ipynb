{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":8897601,"sourceType":"datasetVersion","datasetId":5297895},{"sourceId":8926343,"sourceType":"datasetVersion","datasetId":5369301},{"sourceId":13961094,"sourceType":"datasetVersion","datasetId":8899505},{"sourceId":13970828,"sourceType":"datasetVersion","datasetId":8906538},{"sourceId":13983906,"sourceType":"datasetVersion","datasetId":8913398},{"sourceId":75103,"sourceType":"modelInstanceVersion","modelInstanceId":63082,"modelId":86587}],"dockerImageVersionId":30733,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":148.347272,"end_time":"2024-07-10T01:15:35.655682","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-07-10T01:13:07.30841","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"0f59addf0d2f40309e025976c382cad8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_268e3946106b4e41849bf11c5a375dac","placeholder":"​","style":"IPY_MODEL_5bb130c471af4927a6644f932ae47523","value":" 2/2 [00:03&lt;00:00,  1.48s/it]"}},"19ef2d43bafa44a8b20dc5230aea5ae4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ca042ffa14e4dbebdc66435f7b1f07f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19ef2d43bafa44a8b20dc5230aea5ae4","placeholder":"​","style":"IPY_MODEL_d8a7714cd80d479e859b6ae31ebce7e5","value":"Loading checkpoint shards: 100%"}},"1d03719518b8423099b8b68a92e449d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64ef70cfa9c04764868fa52963323322","placeholder":"​","style":"IPY_MODEL_5e81b324ca1b46a2a96d02bb2acadc0a","value":"Loading checkpoint shards: 100%"}},"1d89705c74d34016bbc1e0601ead825c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d409334237014614bf9ae742597d98ea","placeholder":"​","style":"IPY_MODEL_875758123e4f41f0b5c3fa0cd4fb47c6","value":" 2/2 [01:18&lt;00:00, 34.68s/it]"}},"1ef6d64d40d8461d9e6adddd513089b8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"268e3946106b4e41849bf11c5a375dac":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"324c2396f44f45c89d9ec264007ef9ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5bb130c471af4927a6644f932ae47523":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d243712a1a545e99fa858b0cf19831d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e81b324ca1b46a2a96d02bb2acadc0a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64ef70cfa9c04764868fa52963323322":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68ae4b02a13f4570ad729c437ebd28ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1ca042ffa14e4dbebdc66435f7b1f07f","IPY_MODEL_81b4a9a7cde64b17856b89dbd238c0ef","IPY_MODEL_0f59addf0d2f40309e025976c382cad8"],"layout":"IPY_MODEL_9422a87b93ab4473913e601da3a18689"}},"81b4a9a7cde64b17856b89dbd238c0ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ef6d64d40d8461d9e6adddd513089b8","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_324c2396f44f45c89d9ec264007ef9ff","value":2}},"85d217d6b45847869cea506db59e8b42":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d243712a1a545e99fa858b0cf19831d","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_96385fd98f304649ab5c4ae81333fb63","value":2}},"875758123e4f41f0b5c3fa0cd4fb47c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9422a87b93ab4473913e601da3a18689":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96385fd98f304649ab5c4ae81333fb63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d409334237014614bf9ae742597d98ea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d576a283e6424206ab4c25d809241c21":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8a7714cd80d479e859b6ae31ebce7e5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d98918fae8174629b4819a1114f21202":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1d03719518b8423099b8b68a92e449d7","IPY_MODEL_85d217d6b45847869cea506db59e8b42","IPY_MODEL_1d89705c74d34016bbc1e0601ead825c"],"layout":"IPY_MODEL_d576a283e6424206ab4c25d809241c21"}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## What this notebook is\n\nThis is a inference notebook using 4-bit quantized [Gemma-2 9b Instruct](https://blog.google/technology/developers/google-gemma-2/) and a LoRA adapter trained using the script I uploaded [here](https://www.kaggle.com/code/emiz6413/gemma-2-9b-4-bit-qlora-finetune).\nAlthough we can choose to merge the LoRA adapter to the base model for faster inference, naively doing so could introduce non-negligible quantization error. Therefore, I opted to keep the LoRA adapter unmerged. \n\n## Result\n\n| subset | log loss |\n| - | - |\n| eval set | 0.9371 |\n| public LB | 0.941 |\n\nThe submission takes around 4 hours with `max_length=2048` without TTA.","metadata":{}},{"cell_type":"code","source":"!pip install transformers peft accelerate bitsandbytes \\\n    -U --no-index --find-links /kaggle/input/lmsys-wheel-files","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"papermill":{"duration":31.479497,"end_time":"2024-07-10T01:13:41.690971","exception":false,"start_time":"2024-07-10T01:13:10.211474","status":"completed"},"scrolled":true,"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T05:01:31.033327Z","iopub.execute_input":"2025-12-04T05:01:31.033801Z","iopub.status.idle":"2025-12-04T05:01:55.688760Z","shell.execute_reply.started":"2025-12-04T05:01:31.033775Z","shell.execute_reply":"2025-12-04T05:01:55.687905Z"}},"outputs":[{"name":"stdout","text":"Looking in links: /kaggle/input/lmsys-wheel-files\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nProcessing /kaggle/input/lmsys-wheel-files/transformers-4.42.3-py3-none-any.whl\nProcessing /kaggle/input/lmsys-wheel-files/peft-0.11.1-py3-none-any.whl\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nProcessing /kaggle/input/lmsys-wheel-files/accelerate-0.32.1-py3-none-any.whl\nProcessing /kaggle/input/lmsys-wheel-files/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nInstalling collected packages: bitsandbytes, accelerate, transformers, peft\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.30.1\n    Uninstalling accelerate-0.30.1:\n      Successfully uninstalled accelerate-0.30.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Uninstalling transformers-4.41.2:\n      Successfully uninstalled transformers-4.41.2\nSuccessfully installed accelerate-0.32.1 bitsandbytes-0.43.1 peft-0.11.1 transformers-4.42.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import time\nfrom dataclasses import dataclass\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport torch\nimport sklearn\nimport numpy as np\nimport pandas as pd\nfrom transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig\nfrom transformers.data.data_collator import pad_without_fast_tokenizer_warning\nfrom peft import PeftModel","metadata":{"papermill":{"duration":19.200405,"end_time":"2024-07-10T01:14:00.90474","exception":false,"start_time":"2024-07-10T01:13:41.704335","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T05:01:55.690674Z","iopub.execute_input":"2025-12-04T05:01:55.691123Z","iopub.status.idle":"2025-12-04T05:02:11.469175Z","shell.execute_reply.started":"2025-12-04T05:01:55.691088Z","shell.execute_reply":"2025-12-04T05:02:11.468489Z"}},"outputs":[{"name":"stderr","text":"2025-12-04 05:02:02.081783: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-12-04 05:02:02.081927: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-12-04 05:02:02.199728: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!ls -R /kaggle/input","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T05:02:11.470112Z","iopub.execute_input":"2025-12-04T05:02:11.470567Z","iopub.status.idle":"2025-12-04T05:02:12.730280Z","shell.execute_reply.started":"2025-12-04T05:02:11.470520Z","shell.execute_reply":"2025-12-04T05:02:12.729362Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input:\n73zap2gx  gemma-2\t       lmsys-wheel-files\nd\t  lmsys-chatbot-arena  my-gemma2-lmsys-lora\n\n/kaggle/input/73zap2gx:\ncheckpoint-1000  checkpoint-2400  checkpoint-400   checkpoint-5400\ncheckpoint-1200  checkpoint-2600  checkpoint-4000  checkpoint-5600\ncheckpoint-1400  checkpoint-2800  checkpoint-4200  checkpoint-5748\ncheckpoint-1600  checkpoint-3000  checkpoint-4400  checkpoint-600\ncheckpoint-1800  checkpoint-3200  checkpoint-4600  checkpoint-800\ncheckpoint-200\t checkpoint-3400  checkpoint-4800\ncheckpoint-2000  checkpoint-3600  checkpoint-5000\ncheckpoint-2200  checkpoint-3800  checkpoint-5200\n\n/kaggle/input/73zap2gx/checkpoint-1000:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-1200:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-1400:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-1600:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-1800:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-200:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-2000:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-2200:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-2400:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-2600:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-2800:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-3000:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-3200:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-3400:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-3600:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-3800:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-400:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-4000:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-4200:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-4400:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-4600:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-4800:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-5000:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-5200:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-5400:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-5600:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-5748:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-600:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/73zap2gx/checkpoint-800:\nREADME.md\t\t   rng_state.pth\t    tokenizer.model\nadapter_config.json\t   scheduler.pt\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json  trainer_state.json\noptimizer.pt\t\t   tokenizer.json\t    training_args.bin\n\n/kaggle/input/d:\nchingyong99\n\n/kaggle/input/d/chingyong99:\ngemma-2\n\n/kaggle/input/d/chingyong99/gemma-2:\ngemma-2-9b-it-4bit\n\n/kaggle/input/d/chingyong99/gemma-2/gemma-2-9b-it-4bit:\nconfig.json\t\t\t  special_tokens_map.json\nmodel-00001-of-00002.safetensors  tokenizer.json\nmodel-00002-of-00002.safetensors  tokenizer.model\nmodel.safetensors.index.json\t  tokenizer_config.json\n\n/kaggle/input/gemma-2:\ntransformers\n\n/kaggle/input/gemma-2/transformers:\ngemma-2-9b-it-4bit\n\n/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit:\n1\n\n/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1:\ngemma-2-9b-it-4bit\n\n/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit:\nconfig.json\t\t\t  special_tokens_map.json\nmodel-00001-of-00002.safetensors  tokenizer.json\nmodel-00002-of-00002.safetensors  tokenizer.model\nmodel.safetensors.index.json\t  tokenizer_config.json\n\n/kaggle/input/lmsys-chatbot-arena:\nsample_submission.csv  test.csv  train.csv\n\n/kaggle/input/lmsys-wheel-files:\nMarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nPyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\naccelerate-0.32.1-py3-none-any.whl\nbitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl\ncertifi-2024.7.4-py3-none-any.whl\ncharset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nfilelock-3.15.4-py3-none-any.whl\nfsspec-2024.6.1-py3-none-any.whl\nhuggingface_hub-0.23.4-py3-none-any.whl\nidna-3.7-py3-none-any.whl\njinja2-3.1.4-py3-none-any.whl\nmpmath-1.3.0-py3-none-any.whl\nnetworkx-3.3-py3-none-any.whl\nnumpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nnvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl\nnvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\nnvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\nnvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\nnvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl\nnvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl\nnvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl\nnvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl\nnvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl\nnvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl\nnvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl\nnvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\npackaging-24.1-py3-none-any.whl\npeft-0.11.1-py3-none-any.whl\npsutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nregex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nrequests-2.32.3-py3-none-any.whl\nsafetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nsympy-1.12.1-py3-none-any.whl\ntokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\ntorch-2.3.1-cp310-cp310-manylinux1_x86_64.whl\ntqdm-4.66.4-py3-none-any.whl\ntransformers-4.42.3-py3-none-any.whl\ntriton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\ntyping_extensions-4.12.2-py3-none-any.whl\nurllib3-2.2.2-py3-none-any.whl\n\n/kaggle/input/my-gemma2-lmsys-lora:\nREADME.md\t\t   rng_state.pth\t    tokenizer_config.json\nadapter_config.json\t   scaler.pt\t\t    trainer_state.json\nadapter_model.safetensors  scheduler.pt\t\t    training_args.bin\nchat_template.jinja\t   special_tokens_map.json\noptimizer.pt\t\t   tokenizer.json\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import peft\nprint(\"runtime peft version:\", peft.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T05:02:12.732345Z","iopub.execute_input":"2025-12-04T05:02:12.732644Z","iopub.status.idle":"2025-12-04T05:02:12.737680Z","shell.execute_reply.started":"2025-12-04T05:02:12.732617Z","shell.execute_reply":"2025-12-04T05:02:12.736905Z"}},"outputs":[{"name":"stdout","text":"runtime peft version: 0.11.1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"assert torch.cuda.device_count() == 2","metadata":{"papermill":{"duration":0.047799,"end_time":"2024-07-10T01:14:00.965921","exception":false,"start_time":"2024-07-10T01:14:00.918122","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T05:02:12.738719Z","iopub.execute_input":"2025-12-04T05:02:12.738972Z","iopub.status.idle":"2025-12-04T05:02:12.774158Z","shell.execute_reply.started":"2025-12-04T05:02:12.738952Z","shell.execute_reply":"2025-12-04T05:02:12.773354Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Configurations","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass Config:\n    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'\n    lora_dir = '/kaggle/input/my-gemma2-lmsys-lora'\n    max_length = 2048\n    batch_size = 4\n    device = torch.device(\"cuda\")    \n    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>\n    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input\n\ncfg = Config()","metadata":{"papermill":{"duration":0.021338,"end_time":"2024-07-10T01:14:01.000606","exception":false,"start_time":"2024-07-10T01:14:00.979268","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T05:02:12.775158Z","iopub.execute_input":"2025-12-04T05:02:12.775376Z","iopub.status.idle":"2025-12-04T05:02:12.780384Z","shell.execute_reply.started":"2025-12-04T05:02:12.775357Z","shell.execute_reply":"2025-12-04T05:02:12.779595Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Load & pre-process Data ","metadata":{"papermill":{"duration":0.012663,"end_time":"2024-07-10T01:14:01.026248","exception":false,"start_time":"2024-07-10T01:14:01.013585","status":"completed"},"tags":[]}},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')","metadata":{"papermill":{"duration":0.02967,"end_time":"2024-07-10T01:14:01.06946","exception":false,"start_time":"2024-07-10T01:14:01.03979","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T05:02:12.781491Z","iopub.execute_input":"2025-12-04T05:02:12.782025Z","iopub.status.idle":"2025-12-04T05:02:12.797457Z","shell.execute_reply.started":"2025-12-04T05:02:12.782006Z","shell.execute_reply":"2025-12-04T05:02:12.796889Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def process_text(text: str) -> str:\n    return \" \".join(eval(text, {\"null\": \"\"}))\n\ntest.loc[:, 'prompt'] = test['prompt'].apply(process_text)\ntest.loc[:, 'response_a'] = test['response_a'].apply(process_text)\ntest.loc[:, 'response_b'] = test['response_b'].apply(process_text)\n\ndisplay(test.head(5))","metadata":{"papermill":{"duration":0.040127,"end_time":"2024-07-10T01:14:01.12241","exception":false,"start_time":"2024-07-10T01:14:01.082283","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T05:02:12.798295Z","iopub.execute_input":"2025-12-04T05:02:12.798527Z","iopub.status.idle":"2025-12-04T05:02:12.818647Z","shell.execute_reply.started":"2025-12-04T05:02:12.798508Z","shell.execute_reply":"2025-12-04T05:02:12.817769Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"        id                                             prompt  \\\n0   136060  I have three oranges today, I ate an orange ye...   \n1   211333  You are a mediator in a heated political debat...   \n2  1233961  How to initialize the classification head when...   \n\n                                          response_a  \\\n0                        You have two oranges today.   \n1  Thank you for sharing the details of the situa...   \n2  When you want to initialize the classification...   \n\n                                          response_b  \n0  You still have three oranges. Eating an orange...  \n1  Mr Reddy and Ms Blue both have valid points in...  \n2  To initialize the classification head when per...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>136060</td>\n      <td>I have three oranges today, I ate an orange ye...</td>\n      <td>You have two oranges today.</td>\n      <td>You still have three oranges. Eating an orange...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>211333</td>\n      <td>You are a mediator in a heated political debat...</td>\n      <td>Thank you for sharing the details of the situa...</td>\n      <td>Mr Reddy and Ms Blue both have valid points in...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1233961</td>\n      <td>How to initialize the classification head when...</td>\n      <td>When you want to initialize the classification...</td>\n      <td>To initialize the classification head when per...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"# Tokenize","metadata":{"papermill":{"duration":0.012864,"end_time":"2024-07-10T01:14:01.148412","exception":false,"start_time":"2024-07-10T01:14:01.135548","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def tokenize(\n    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n):\n    prompt = [\"<prompt>: \" + p for p in prompt]\n    response_a = [\"\\n\\n<response_a>: \" + r_a for r_a in response_a]\n    response_b = [\"\\n\\n<response_b>: \" + r_b for r_b in response_b]\n    if spread_max_length:\n        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n        attention_mask = [[1]* len(i) for i in input_ids]\n    else:\n        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n        input_ids = tokenized.input_ids\n        attention_mask = tokenized.attention_mask\n    return input_ids, attention_mask","metadata":{"papermill":{"duration":0.030237,"end_time":"2024-07-10T01:14:01.194318","exception":false,"start_time":"2024-07-10T01:14:01.164081","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T05:02:12.820037Z","iopub.execute_input":"2025-12-04T05:02:12.820462Z","iopub.status.idle":"2025-12-04T05:02:12.827187Z","shell.execute_reply.started":"2025-12-04T05:02:12.820434Z","shell.execute_reply":"2025-12-04T05:02:12.826360Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"%%time\n\ntokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)\ntokenizer.add_eos_token = True\ntokenizer.padding_side = \"right\"\n\ndata = pd.DataFrame()\ndata[\"id\"] = test[\"id\"]\ndata[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\ndata[\"length\"] = data[\"input_ids\"].apply(len)\n\naug_data = pd.DataFrame()\naug_data[\"id\"] = test[\"id\"]\n# swap response_a & response_b\naug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\naug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)","metadata":{"papermill":{"duration":1.169844,"end_time":"2024-07-10T01:14:02.377579","exception":false,"start_time":"2024-07-10T01:14:01.207735","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T05:02:12.830395Z","iopub.execute_input":"2025-12-04T05:02:12.830721Z","iopub.status.idle":"2025-12-04T05:02:13.953771Z","shell.execute_reply.started":"2025-12-04T05:02:12.830693Z","shell.execute_reply":"2025-12-04T05:02:13.952841Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 748 ms, sys: 153 ms, total: 900 ms\nWall time: 1.11 s\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"print(tokenizer.decode(data[\"input_ids\"][0]))","metadata":{"papermill":{"duration":0.024759,"end_time":"2024-07-10T01:14:02.419091","exception":false,"start_time":"2024-07-10T01:14:02.394332","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T05:02:13.955193Z","iopub.execute_input":"2025-12-04T05:02:13.956031Z","iopub.status.idle":"2025-12-04T05:02:13.962169Z","shell.execute_reply.started":"2025-12-04T05:02:13.955989Z","shell.execute_reply":"2025-12-04T05:02:13.961234Z"}},"outputs":[{"name":"stdout","text":"<bos><prompt>: I have three oranges today, I ate an orange yesterday. How many oranges do I have?\n\n<response_a>: You have two oranges today.\n\n<response_b>: You still have three oranges. Eating an orange yesterday does not affect the number of oranges you have today.<eos>\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(tokenizer.decode(aug_data[\"input_ids\"][0]))","metadata":{"papermill":{"duration":0.021982,"end_time":"2024-07-10T01:14:02.454045","exception":false,"start_time":"2024-07-10T01:14:02.432063","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T05:02:13.963227Z","iopub.execute_input":"2025-12-04T05:02:13.963761Z","iopub.status.idle":"2025-12-04T05:02:13.972746Z","shell.execute_reply.started":"2025-12-04T05:02:13.963727Z","shell.execute_reply":"2025-12-04T05:02:13.971978Z"}},"outputs":[{"name":"stdout","text":"<bos><prompt>: I have three oranges today, I ate an orange yesterday. How many oranges do I have?\n\n<response_a>: You still have three oranges. Eating an orange yesterday does not affect the number of oranges you have today.\n\n<response_b>: You have two oranges today.<eos>\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Load model","metadata":{"papermill":{"duration":0.013054,"end_time":"2024-07-10T01:14:02.480304","exception":false,"start_time":"2024-07-10T01:14:02.46725","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Load base model on GPU 0\ndevice_0 = torch.device('cuda:0')\nmodel_0 = Gemma2ForSequenceClassification.from_pretrained(\n    cfg.gemma_dir,\n    device_map=device_0,\n    use_cache=False,\n)\n\n# Load base model on GPU 1\ndevice_1 = torch.device('cuda:1')\nmodel_1 = Gemma2ForSequenceClassification.from_pretrained(\n    cfg.gemma_dir,\n    device_map=device_1,\n    use_cache=False,\n)","metadata":{"papermill":{"duration":83.919146,"end_time":"2024-07-10T01:15:26.412583","exception":false,"start_time":"2024-07-10T01:14:02.493437","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T05:02:13.973837Z","iopub.execute_input":"2025-12-04T05:02:13.974113Z","iopub.status.idle":"2025-12-04T05:03:59.403412Z","shell.execute_reply.started":"2025-12-04T05:02:13.974095Z","shell.execute_reply":"2025-12-04T05:03:59.402491Z"}},"outputs":[{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95c78f09e6ec446399310f31bbee9a84"}},"metadata":{}},{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e24d06bd2c53422382adbfcebc7691d9"}},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"#### Load LoRA adapter","metadata":{"papermill":{"duration":0.013639,"end_time":"2024-07-10T01:15:26.440571","exception":false,"start_time":"2024-07-10T01:15:26.426932","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)\nmodel_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)","metadata":{"papermill":{"duration":1.265087,"end_time":"2024-07-10T01:15:27.719297","exception":false,"start_time":"2024-07-10T01:15:26.45421","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T05:03:59.404587Z","iopub.execute_input":"2025-12-04T05:03:59.404873Z","iopub.status.idle":"2025-12-04T05:04:00.398082Z","shell.execute_reply.started":"2025-12-04T05:03:59.404829Z","shell.execute_reply":"2025-12-04T05:04:00.397366Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Inference\n","metadata":{"papermill":{"duration":0.013989,"end_time":"2024-07-10T01:15:27.797512","exception":false,"start_time":"2024-07-10T01:15:27.783523","status":"completed"},"tags":[]}},{"cell_type":"code","source":"@torch.no_grad()\n@torch.cuda.amp.autocast()\ndef inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n    a_win, b_win, tie = [], [], []\n    \n    for start_idx in range(0, len(df), batch_size):\n        end_idx = min(start_idx + batch_size, len(df))\n        tmp = df.iloc[start_idx:end_idx]\n        input_ids = tmp[\"input_ids\"].to_list()\n        attention_mask = tmp[\"attention_mask\"].to_list()\n        inputs = pad_without_fast_tokenizer_warning(\n            tokenizer,\n            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n            padding=\"longest\",\n            pad_to_multiple_of=None,\n            return_tensors=\"pt\",\n        )\n        outputs = model(**inputs.to(device))\n        proba = outputs.logits.softmax(-1).cpu()\n        \n        a_win.extend(proba[:, 0].tolist())\n        b_win.extend(proba[:, 1].tolist())\n        tie.extend(proba[:, 2].tolist())\n    \n    df[\"winner_model_a\"] = a_win\n    df[\"winner_model_b\"] = b_win\n    df[\"winner_tie\"] = tie\n    \n    return df","metadata":{"papermill":{"duration":0.026726,"end_time":"2024-07-10T01:15:27.838497","exception":false,"start_time":"2024-07-10T01:15:27.811771","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T05:04:00.399124Z","iopub.execute_input":"2025-12-04T05:04:00.399386Z","iopub.status.idle":"2025-12-04T05:04:00.406517Z","shell.execute_reply.started":"2025-12-04T05:04:00.399364Z","shell.execute_reply":"2025-12-04T05:04:00.405754Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"st = time.time()\n\n# sort by input length to fully leverage dynaminc padding\ndata = data.sort_values(\"length\", ascending=False)\n# the total #tokens in sub_1 and sub_2 should be more or less the same\nsub_1 = data.iloc[0::2].copy()\nsub_2 = data.iloc[1::2].copy()\n\nwith ThreadPoolExecutor(max_workers=2) as executor:\n    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n\nresult_df = pd.concat(list(results), axis=0)\nproba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n\nprint(f\"elapsed time: {time.time() - st}\")","metadata":{"papermill":{"duration":4.598663,"end_time":"2024-07-10T01:15:32.45234","exception":false,"start_time":"2024-07-10T01:15:27.853677","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T05:04:00.407631Z","iopub.execute_input":"2025-12-04T05:04:00.407871Z","iopub.status.idle":"2025-12-04T05:04:05.246642Z","shell.execute_reply.started":"2025-12-04T05:04:00.407833Z","shell.execute_reply":"2025-12-04T05:04:05.245755Z"}},"outputs":[{"name":"stdout","text":"elapsed time: 4.827924013137817\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"st = time.time()\n\nif cfg.tta:\n    data = aug_data.sort_values(\"length\", ascending=False)  # sort by input length to boost speed\n    sub_1 = data.iloc[0::2].copy()\n    sub_2 = data.iloc[1::2].copy()\n\n    with ThreadPoolExecutor(max_workers=2) as executor:\n        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n\n    tta_result_df = pd.concat(list(results), axis=0)\n    # recall TTA's order is flipped\n    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values \n    # average original result and TTA result.\n    proba = (proba + tta_proba) / 2\n\nprint(f\"elapsed time: {time.time() - st}\")","metadata":{"papermill":{"duration":0.024559,"end_time":"2024-07-10T01:15:32.491283","exception":false,"start_time":"2024-07-10T01:15:32.466724","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T05:04:05.247882Z","iopub.execute_input":"2025-12-04T05:04:05.248162Z","iopub.status.idle":"2025-12-04T05:04:05.255309Z","shell.execute_reply.started":"2025-12-04T05:04:05.248141Z","shell.execute_reply":"2025-12-04T05:04:05.254157Z"}},"outputs":[{"name":"stdout","text":"elapsed time: 0.0001952648162841797\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"result_df.loc[:, \"winner_model_a\"] = proba[:, 0]\nresult_df.loc[:, \"winner_model_b\"] = proba[:, 1]\nresult_df.loc[:, \"winner_tie\"] = proba[:, 2]\nsubmission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\nsubmission_df.to_csv('submission.csv', index=False)\ndisplay(submission_df)","metadata":{"papermill":{"duration":0.034664,"end_time":"2024-07-10T01:15:32.539974","exception":false,"start_time":"2024-07-10T01:15:32.50531","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T05:04:05.256472Z","iopub.execute_input":"2025-12-04T05:04:05.256799Z","iopub.status.idle":"2025-12-04T05:04:05.280582Z","shell.execute_reply.started":"2025-12-04T05:04:05.256778Z","shell.execute_reply":"2025-12-04T05:04:05.279825Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"        id  winner_model_a  winner_model_b  winner_tie\n2  1233961        0.239998        0.478197    0.281805\n0   136060        0.016983        0.929061    0.053956\n1   211333        0.219016        0.392589    0.388395","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>1233961</td>\n      <td>0.239998</td>\n      <td>0.478197</td>\n      <td>0.281805</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>136060</td>\n      <td>0.016983</td>\n      <td>0.929061</td>\n      <td>0.053956</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>211333</td>\n      <td>0.219016</td>\n      <td>0.392589</td>\n      <td>0.388395</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":18}]}